{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "    1. Introduction\n",
    "\n",
    "    2. Dataset Preparation\n",
    "    \n",
    "    3. Exploratory Data Analysis\n",
    "\n",
    "    4.  KNN Model\n",
    "\n",
    "        4.1 Hyperparameter Tuning on  KNN Algorithm\n",
    "        4.2 Building RecSys based on KNN\n",
    "        4.3 Evaulation of KNN Model \n",
    "    \n",
    "\n",
    "    5. SVD Model\n",
    "\n",
    "        5.1 Building RecSys based on SVD\n",
    "        5.2 Evaulation of SVD Model\n",
    "\n",
    "    6. Answering Business Questions \n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, Recommender Systems (RecSys) have become an essential tool for companies and websites to personalize their content and improve their user experience. Collaborative filtering techniques are widely used in building RecSys, and two of the most popular methods are K-Nearest Neighbors (KNN) and Singular Value Decomposition (SVD). In this assignment, I will be using the Jester dataset, which contains user ratings for jokes. \n",
    "The aim is to build two different RecSys based on KNN and SVD, respectively, to recommend new jokes to users based on their previous ratings. The KNN technique finds the k most similar users to a given user and uses their ratings to make joke recommendations, while the SVD model factorizes the user-joke matrix into lower-rank matrices to make recommendations based on similar jokes to the user's highest-rated joke. This assignment is  conducted by Dilay Durukan, a student of the Master's programme Digital Driven Business, as part of the course System Development for Marketingâ€™. This assignment will provide valuable insights into the application of collaborative filtering techniques in building RecSys."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following main Business question has been formulated: \n",
    "\n",
    "\n",
    " **How can we improve the accuracy of joke recommendations on our  platform by providing personalized joke recommendations based on their past ratings?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To answer this main Business questions the following five sub-questions have been defined:\n",
    "\n",
    "\n",
    "1. How do users rate jokes on our platform and what is the distribution of joke ratings ?\n",
    "\n",
    "2. What kind of techniques can be used for recommendation systems for joke recommendations?\n",
    "\n",
    "3. Which algorithm can be used for identifying groups of users with similar joke preferences and recommend joke based on those groups?\n",
    "\n",
    "4. Which algorithm can be used for identifying similar joke for persons favorite joke and recommend joke based on that?\n",
    "\n",
    "5. How accurate are our KNN and SVD models in predicting user ratings for jokes?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "import scipy.stats as stats\n",
    "import IPython\n",
    "import glob, os\n",
    "from wordcloud import WordCloud\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.linalg import svd\n",
    "from IPython.display import display\n",
    "import textwrap\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the datasets with jokes data\n",
    "df = pd.read_csv(\"jester_ratings.csv\") \n",
    "df2 = pd.read_csv(\"jester_items.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>jokeId</th>\n",
       "      <th>rating</th>\n",
       "      <th>jokeText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>Q.\\tWhat's O. J. Simpson's Internet address? \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.281</td>\n",
       "      <td>How many feminists does it take to screw in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-9.281</td>\n",
       "      <td>Q. Did you hear about the dyslexic devil worsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-6.781</td>\n",
       "      <td>They asked the Japanese visitor if they have e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.875</td>\n",
       "      <td>Q:  What did the blind person say when given s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  jokeId  rating                                           jokeText\n",
       "0       1       5   0.219  Q.\\tWhat's O. J. Simpson's Internet address? \\...\n",
       "1       1       7  -9.281  How many feminists does it take to screw in a ...\n",
       "2       1       8  -9.281  Q. Did you hear about the dyslexic devil worsh...\n",
       "3       1      13  -6.781  They asked the Japanese visitor if they have e...\n",
       "4       1      15   0.875  Q:  What did the blind person say when given s..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging datasets according to  \"jokeId\" \n",
    "df = df.merge(right=df2[['jokeId','jokeText']], how='left',on='jokeId')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId      0\n",
       "jokeId      0\n",
       "rating      0\n",
       "jokeText    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId        int64\n",
       "jokeId        int64\n",
       "rating      float64\n",
       "jokeText     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1761439, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId      59132\n",
       "jokeId        140\n",
       "rating        641\n",
       "jokeText      140\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1757560"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['rating'].astype(bool)).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 59132 different users, 140 jokes and 1757560  ratings ranging from -10 to +10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>jokeId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.761439e+06</td>\n",
       "      <td>1.761439e+06</td>\n",
       "      <td>1.761439e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.272322e+04</td>\n",
       "      <td>7.071133e+01</td>\n",
       "      <td>1.618602e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.828011e+04</td>\n",
       "      <td>4.600790e+01</td>\n",
       "      <td>5.302608e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>-1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.720200e+04</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>-2.031000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.480800e+04</td>\n",
       "      <td>6.900000e+01</td>\n",
       "      <td>2.219000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.730600e+04</td>\n",
       "      <td>1.120000e+02</td>\n",
       "      <td>5.719000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.397800e+04</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             userId        jokeId        rating\n",
       "count  1.761439e+06  1.761439e+06  1.761439e+06\n",
       "mean   3.272322e+04  7.071133e+01  1.618602e+00\n",
       "std    1.828011e+04  4.600790e+01  5.302608e+00\n",
       "min    1.000000e+00  5.000000e+00 -1.000000e+01\n",
       "25%    1.720200e+04  2.100000e+01 -2.031000e+00\n",
       "50%    3.480800e+04  6.900000e+01  2.219000e+00\n",
       "75%    4.730600e+04  1.120000e+02  5.719000e+00\n",
       "max    6.397800e+04  1.500000e+02  1.000000e+01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>jokeId</th>\n",
       "      <th>rating</th>\n",
       "      <th>jokeText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>Q.\\tWhat's O. J. Simpson's Internet address? \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.281</td>\n",
       "      <td>How many feminists does it take to screw in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-9.281</td>\n",
       "      <td>Q. Did you hear about the dyslexic devil worsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-6.781</td>\n",
       "      <td>They asked the Japanese visitor if they have e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.875</td>\n",
       "      <td>Q:  What did the blind person say when given s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  jokeId  rating                                           jokeText\n",
       "0       1       5   0.219  Q.\\tWhat's O. J. Simpson's Internet address? \\...\n",
       "1       1       7  -9.281  How many feminists does it take to screw in a ...\n",
       "2       1       8  -9.281  Q. Did you hear about the dyslexic devil worsh...\n",
       "3       1      13  -6.781  They asked the Japanese visitor if they have e...\n",
       "4       1      15   0.875  Q:  What did the blind person say when given s..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#In order to run the code faster, data is filtered based on some condition on popularity\n",
    "\n",
    "min_joke = 100   # joke has to have been rated over 100 times\n",
    "min_user = 50  # user has to have rated at least 50 times\n",
    "\n",
    "users = df.groupby('userId')['rating'].count()\n",
    "users = users.loc[users > min_user].index.values\n",
    "jokes = df.groupby('jokeId')['rating'].count()\n",
    "jokes = jokes.loc[jokes > min_joke].index.values\n",
    "df_filtered = df.loc[df.userId.isin(users) & df.jokeId.isin(jokes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered:  1761439\n",
      "Filtered:  947736\n",
      "Kept 54.0% of data\n"
     ]
    }
   ],
   "source": [
    "print('Unfiltered: ', df.shape[0])\n",
    "print('Filtered: ', df_filtered.shape[0])\n",
    "print('Kept {}% of data'.format(round(df_filtered.shape[0]/df.shape[0], 2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"Paired\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# The Most Common Word in Joke Texts Overwiews\\nplt.figure(figsize=(18,18))\\nplt.title('The Most Common Word in Joke Texts Overviews\\n', fontsize=30, weight=500, color='green')\\nwc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['jokeText']))\\n\\nplt.imshow(wc)\\nplt.axis('off')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# The Most Common Word in Joke Texts Overwiews\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.title('The Most Common Word in Joke Texts Overviews\\n', fontsize=30, weight=500, color='green')\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['jokeText']))\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Top Rated Jokes Plot\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Group the jokes by ID and compute the mean rating for each joke\n",
    "joke_ratings = df.groupby(['jokeId', 'jokeText'])['rating'].mean()\n",
    "\n",
    "# Sort the jokes by mean rating and select the top 10\n",
    "top_jokes = joke_ratings.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Extract the joke texts and mean ratings for the top 10 jokes\n",
    "joke_texts = top_jokes.index.get_level_values('jokeText').tolist()\n",
    "mean_ratings = top_jokes.values.tolist()\n",
    "\n",
    "# Wrap long text into two lines\n",
    "joke_texts = [textwrap.fill(text, width=40).split('\\n')[:2] for text in joke_texts]\n",
    "joke_texts = ['\\n'.join(text) for text in joke_texts]\n",
    "joke_texts\n",
    "\n",
    "# Plot the top rated jokes\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(joke_texts, mean_ratings, align='center',color=colors)\n",
    "ax.set_xlabel('Mean Rating')\n",
    "ax.set_ylabel('Joke Text')\n",
    "ax.set_title('Top 5 Highest Rated Jokes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count of Jokes for Each Rating Plot\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "df['Round_Rating'] = round(df['rating'])\n",
    "\n",
    "# Group the jokes by rating and count the number of jokes in each rating group\n",
    "rating_counts = df.groupby('Round_Rating')['jokeId'].count()\n",
    "\n",
    "# Plot the count of jokes for each rating\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(rating_counts.index, rating_counts.values,color=colors)\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Count of Jokes')\n",
    "ax.set_title('Count of Jokes for Each Rating')\n",
    "\n",
    "# Add the count of jokes on top of each bar\n",
    "for i, v in enumerate(rating_counts.values):\n",
    "    ax.text(i-10, v + 10, str(v), ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. KNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this assignment, a RecSys has been built using the Jester Price dataset, which contains user ratings for jokes. \n",
    "\n",
    "The KNN model built in this assignment is based on the **user-user similarity**.\n",
    "\n",
    "The implemented RecSys uses the KNN algorithm to find the most similar users to a given input user and recommends a joke based on ratings of the similar users.\n",
    "\n",
    "The KNN algorithm works by comparing the ratings of the input user with those of other users in the dataset, and finding the k most similar users. The similarity between users is determined by the distance between their rating vectors. Once the k most similar users have been identified, the RecSys looks at the jokes rated by these users and recommends the joke with the highest average rating among them.\n",
    "\n",
    "The system is built on the Jester  dataset, which includes joke IDs, user IDs,joke texts  and joke ratings. By leveraging this data, it is able to provide users with recommendations that are tailored to their preferences and rating history.\n",
    "\n",
    "\n",
    "This implementation offers a straightforward and effective approach to recommend jokes to users based on the ratings of other users with similar taste. The RecSys has the potential to enhance user engagement and satisfaction by providing personalized recommendations. The algorithm can also be further improved by implementing different distance metrics, or by incorporating other features such as the text content of jokes, to provide even more accurate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.sort_values(['userId','jokeId',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating User-Joke Matrix\n",
    "jokes_user_matrix=df_filtered.pivot(index='userId', columns= 'jokeId',values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "\n",
    "jokes_user_train, jokes_user_test = train_test_split(jokes_user_matrix, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_data=  df_filtered[df_filtered['userId'].isin(jokes_user_train.index.tolist())]\n",
    "test_data= df_filtered[df_filtered['userId'].isin(jokes_user_test.index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data into a sparse matrix format\n",
    "csrtrain = csr_matrix(jokes_user_train.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.1 Hyperparameter Tuning on  KNN Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of hyperparameter tuning is to find the combination of hyperparameters that produces the best performance on the given dataset. The hyperparameters are values that are set before training the model and cannot be learned during training.  In this assignment hyperparameter tuning is done by searching through a range of possible hyperparameters and evaluating the performance of the model using each combination. The evaluation metric used to measure performance is selected as mean of the distances of nearest neighbors aka similar users. If this metric is low it means that the distances are small and neighbors are similar. If this metric is high then distances to  neighbors are higher and users are less similar. The hyperparameters that result in the best performance on the evaluation metric(minimum distance) are selected as the optimal hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 10   algorithm= brute   metric= cosine   Mean of distances to similar users=  0.3621879565060701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 10   algorithm= brute   metric= euclidean   Mean of distances to similar users=  34.56583181612742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 10   algorithm= auto   metric= cosine   Mean of distances to similar users=  0.3621879565060701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 10   algorithm= auto   metric= euclidean   Mean of distances to similar users=  34.56583181612742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 20   algorithm= brute   metric= cosine   Mean of distances to similar users=  0.39912152778212406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 20   algorithm= brute   metric= euclidean   Mean of distances to similar users=  37.32624753630715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 20   algorithm= auto   metric= cosine   Mean of distances to similar users=  0.39912152778212406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 20   algorithm= auto   metric= euclidean   Mean of distances to similar users=  37.32624753630715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 30   algorithm= brute   metric= cosine   Mean of distances to similar users=  0.41658175522933316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 30   algorithm= brute   metric= euclidean   Mean of distances to similar users=  38.503154477644536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 30   algorithm= auto   metric= cosine   Mean of distances to similar users=  0.41658175522933316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n= 30   algorithm= auto   metric= euclidean   Mean of distances to similar users=  38.503154477644536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3591503917.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Creating an empty dataframe to store the results of hyperparameter tuning for KNN algorithm\n",
    "knn_tuning_results = pd.DataFrame(columns=['n_neighbors', 'algorithm', 'metric','mean_distances'])\n",
    "\n",
    "#Defining the range of hyperparameters to be tuned\n",
    "n_neighbors= [10, 20, 30]\n",
    "algorithm= [ 'brute', 'auto']\n",
    "metric=['cosine', 'euclidean']\n",
    "\n",
    "#Loop over the combinations of hyperparameters to perform the KNN algorithm with each combination\n",
    "for i in n_neighbors:\n",
    "    for j in  algorithm:\n",
    "        for k in  metric:\n",
    "            # Create a KNN object with the current hyperparameter combination\n",
    "            knn = NearestNeighbors(n_neighbors=i,\n",
    "                           algorithm=j,\n",
    "                           metric=k)\n",
    "            # Fit the KNN model with the training data\n",
    "            knn.fit( csrtrain)\n",
    "            # Calculate the distances and indices of similar users\n",
    "            distances, indices = knn.kneighbors(csrtrain, n_neighbors=i)\n",
    "            # Print the mean of distances to similar users for the current hyperparameter combination\n",
    "            print('For n=' , i, '  algorithm=', j, '  metric=' , k , '  Mean of distances to similar users= ' , distances.mean())\n",
    "\n",
    "            # Append the hyperparameter combination and mean distance to the knn_tuning_results dataframe\n",
    "            knn_tuning_result = {'n_neighbors': i, 'algorithm': j,'metric': k, 'mean_distances': distances.mean()}\n",
    "            knn_tuning_results = knn_tuning_results.append(knn_tuning_result, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we can see the distances(errors) for each combination of hyperparameters for the KNN model. In the following code, the optimal hyperparameters are selected according to minimum of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>metric</th>\n",
       "      <th>mean_distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>brute</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0.362188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>brute</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>34.565832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0.362188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>34.565832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>brute</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0.399122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>brute</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>37.326248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>auto</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0.399122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>auto</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>37.326248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>brute</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0.416582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>brute</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>38.503154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>auto</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0.416582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>auto</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>38.503154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_neighbors algorithm     metric mean_distances\n",
       "0           10     brute     cosine       0.362188\n",
       "1           10     brute  euclidean      34.565832\n",
       "2           10      auto     cosine       0.362188\n",
       "3           10      auto  euclidean      34.565832\n",
       "4           20     brute     cosine       0.399122\n",
       "5           20     brute  euclidean      37.326248\n",
       "6           20      auto     cosine       0.399122\n",
       "7           20      auto  euclidean      37.326248\n",
       "8           30     brute     cosine       0.416582\n",
       "9           30     brute  euclidean      38.503154\n",
       "10          30      auto     cosine       0.416582\n",
       "11          30      auto  euclidean      38.503154"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tuning_results['mean_distances'] = knn_tuning_results['mean_distances'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the index of the row with the minimum mean distance\n",
    "index_min_value = knn_tuning_results['mean_distances'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters for the KNN model are n= 10   algorithm= brute   metric= cosine\n"
     ]
    }
   ],
   "source": [
    "# Finding the index of the row with the minimum mean distance\n",
    "index_min_value = knn_tuning_results['mean_distances'].idxmin()\n",
    "\n",
    "#Getting the optimal hyperparameters from the row with the minimum mean distance\n",
    "best_n_neighbors = knn_tuning_results.iloc[index_min_value, 0]\n",
    "best_algorithm = knn_tuning_results.iloc[index_min_value,1]\n",
    "best_metric = knn_tuning_results.iloc[index_min_value, 2]\n",
    "\n",
    "#Printing the optimal hyperparameters\n",
    "print('Optimal hyperparameters for the KNN model are n=' , best_n_neighbors, '  algorithm=', best_algorithm, '  metric=' , best_metric )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After performing hyperparameter tuning on a KNN algorithm, the optimal hyperparameters are determined based on their ability to minimize the error of the model. Since the optimal hyperparameters are determined now, they can be used to train the KNN model with the goal of minimizing the error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.2 Building RecSys based on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine', n_neighbors=10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new NearestNeighbors object with the best hyperparameters found during tuning\n",
    "knn = NearestNeighbors(n_neighbors=best_n_neighbors,\n",
    "                           algorithm=best_algorithm,\n",
    "                           metric=best_metric)\n",
    "\n",
    "# Fitting the KNN model on the training data\n",
    "knn.fit(csrtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8232, 140)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrtrain.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that finds similar users to a given user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes an input_user_index and neighbor_count as inputs. It calculates the k nearest neighbors of the input user based on their interaction history with jokes using the KNN model trained earlier. Then, it prints the most similar users to the input user based on the calculated distances. Finally, it creates a list of similar users based on their indices in the training data and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_users(input_user_index, neighbor_count=best_n_neighbors+1):\n",
    "    \n",
    "    # Finding the k nearest neighbors of the input user \n",
    "    distances, indices = knn.kneighbors(jokes_user_train.iloc[input_user_index,:].values.reshape(1,-1), n_neighbors=neighbor_count)\n",
    "  \n",
    "    # indices ARE INDEX OF THE USER ID\n",
    "    # jokes_user_train.index[indices.flatten()[1]] ARE THE USER ID OF THE INDEX\n",
    "    # Creating a list of similar users \n",
    "\n",
    "    similaruser_ids=[]\n",
    "\n",
    "    # Printing the most similar users to the input user based on the calculated distances\n",
    "    for i in range(0,len(distances.flatten())):\n",
    "        if i==0:            #Most similar user to input user is itself with zero distance. So not printing the input user \n",
    "            print('Similar Users for User',jokes_user_train.index[indices.flatten()[i]],' \\n')\n",
    "        else:\n",
    "            print( 'Most Similar User',i,': User_Id ' +  str(jokes_user_train.index[indices.flatten()[i]]),' with distance of ',str(distances.flatten()[i]) )\n",
    "            similaruser_ids.append(jokes_user_train.index[indices.flatten()[i]])\n",
    "\n",
    "\n",
    "\n",
    "    return similaruser_ids\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that takes a dataframe of jokes rated by similar users and returns the joke with the highest average rating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a pandas DataFrame object 'JokesfromSimilarUsers' as input, which contains a list of jokes and their average rating by similar users. \n",
    "The code first converts the 'jokeId', 'avg_rating_by_similar_users', and 'jokeText' columns of the DataFrame into lists using the 'tolist()' method. \n",
    "Then, it initializes variables to keep track of the maximum rating and the recommended joke. \n",
    "The code loops through the lists to find the joke with the highest rating. For each joke, the code checks whether its rating is greater than the current maximum rating (maxrating). If so, the joke's text (textlist[i]) is stored in the recommended_joke variable and its rating is stored in the maxrating variable. Finally, the function returns the recommended joke with the highest rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Jokes_with_Highhest_Rating_from_Similar_Users(JokesfromSimilarUsers):\n",
    "\n",
    "    # Converting jokeId, jokeText and avg_rating_by_similar_users columns to lists\n",
    "    \n",
    "    jokeidlist=JokesfromSimilarUsers[\"jokeId\"].tolist()\n",
    "    ratelist= JokesfromSimilarUsers[\"avg_rating_by_similar_users\"].tolist()\n",
    "    textlist= JokesfromSimilarUsers[\"jokeText\"].tolist()\n",
    "\n",
    "    # Initialize variables to keep track of maximum rating and recommended joke\n",
    "    maxrating=0\n",
    "    recommended_joke = None\n",
    "\n",
    "    #Loop through the lists to find the joke with the highest rating\n",
    "    for i in range(0,len(jokeidlist)):\n",
    "        if (ratelist[i]> maxrating):\n",
    "            recommended_joke=textlist[i]\n",
    "            maxrating=ratelist[i]\n",
    "\n",
    "    # Returning the recommended joke with the highest rating            \n",
    "    return recommended_joke"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that takes a dataframe of jokes rated by similar users and returns the joke with the highest average rating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function get_KNN_recommandations_for_user takes an input user ID and neighbor count as inputs. It first finds the index of the input user in the train_data dataframe, and then gets the similar users using the get_similar_users function created earlier.\n",
    "\n",
    "Then, it filters out the data for the similar users and calculates the average rating for each joke by these users. It creates a new dataframe JokesfromSimilarUsers which contains unique jokes with their corresponding joke ID, joke text, and average rating by similar users.\n",
    "\n",
    "Next, it removes the jokes that the input user has already rated from the JokesfromSimilarUsers dataframe.\n",
    "\n",
    "If all the jokes rated by similar users are also watched by the input user, then the function recursively calls itself with an increased neighbor user count to find more dissimilar users to generate recommendations.\n",
    "\n",
    "\n",
    "After this, it calls the function get_Jokes_with_Highhest_Rating_from_Similar_Users to get the joke with the highest average rating by similar users from the filtered JokesfromSimilarUsers dataframe.\n",
    "\n",
    "Then, it finds the rating of this joke by similar users and its general rating in the entire dataset, and prints these along with the recommended joke for the input user.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KNN_recommandations_for_user(input_user, neighbor_count=best_n_neighbors+1):\n",
    "   \n",
    "    #Getting the index of given input user in train data\n",
    "    user_ids = train_data['userId'].unique()\n",
    "    user_ids.sort()\n",
    "    user_ids=user_ids.tolist()\n",
    "    input_user_index = user_ids.index(input_user)\n",
    "   \n",
    "    \n",
    "    #Getting similar users to given input user\n",
    "    similaruser=None\n",
    "    similaruser=get_similar_users(input_user_index,neighbor_count)\n",
    "    \n",
    "    #Filtering the data to only include the rows of similar users\n",
    "    similar_user_data=None\n",
    "    similar_user_data = train_data[train_data[\"userId\"].isin(similaruser)]\n",
    "\n",
    "    #Computing the Average Rating of Jokes among the similar users \n",
    "    similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
    "\n",
    "\n",
    "    # Getting jokes rated by similar users and   excluding jokes that already rated by input_user\n",
    "    JokesfromSimilarUsers=None\n",
    "    JokesfromSimilarUsers= similar_user_data[[\"jokeId\",'jokeText','avg_rating_by_similar_users']].drop_duplicates(subset=['jokeId'])\n",
    "    JokesRatedBySelectedUser=train_data[(train_data['userId'] == input_user)]['jokeText'].tolist()\n",
    "    JokesfromSimilarUsers = JokesfromSimilarUsers[~JokesfromSimilarUsers['jokeText'].isin(JokesRatedBySelectedUser)]\n",
    "\n",
    "    # If all the jokes  rated by  similar users are also  rated by the selected user, call the function again with increased  neighbor user count\n",
    "    if (JokesfromSimilarUsers.empty):\n",
    "        print('All the jokes rated  by', (neighbor_count-1),' similar users are all  rated by the selected user. Similar user count  is increased by 5 to ', neighbor_count+5)\n",
    "        return get_KNN_recommandations_for_user(input_user, neighbor_count+5)\n",
    "\n",
    "\n",
    "    # Getting the recommended joke with the highest average rating among similar user\n",
    "    recommended_joke=None\n",
    "    recommended_joke = get_Jokes_with_Highhest_Rating_from_Similar_Users(JokesfromSimilarUsers)\n",
    "\n",
    "\n",
    "    # Assigning rating values to the  variables that desired to print\n",
    "        # get average rating for recommended joke among similar users\n",
    "    RatingbySimilarUsers=JokesfromSimilarUsers[(JokesfromSimilarUsers['jokeText'] == recommended_joke)]['avg_rating_by_similar_users'].iloc[0]\n",
    "        # get general rating for recommended joke in training data\n",
    "    GeneralRating=round(train_data[(train_data['jokeText'] == recommended_joke)].groupby('jokeId')['rating'].transform('mean').iloc[0])\n",
    "    \n",
    "    # Printing the recommendation joke with ratings\n",
    "    print('\\n \\n For User ',input_user, 'recommended joke is: \\n', recommended_joke,' \\n', ' with Rating by Similar Users: ' , RatingbySimilarUsers,' \\n', ' with General Rating: ' , GeneralRating )\n",
    " \n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Getting Joke Recommandation to Random User based on KNN\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code selects a random user id from the list of unique user ids in the train_data dataframe using the np.random.choice() function. The get_KNN_recommandations_for_user() function is then called with this randomly selected user id as the input to generate a joke recommendation for the selected user using the KNN algorithm.\n",
    "\n",
    "The get_KNN_recommandations_for_user() function first finds the index of the input user id in the user_ids list using the index() method. Then, it calls the get_similar_users() function to find a list of similar users to the input user based on their rating patterns. The function then filters out the jokes already rated by the input user and calculates the average rating of the remaining jokes by the similar users. It selects the joke with the highest rating and returns its text as the recommended joke for the input user.\n",
    "\n",
    "Finally, the function also prints the rating of the recommended joke by the similar users and the general rating of the same joke in the entire dataset. This code allows for easy testing of the recommendation algorithm on different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Users for User 38830  \n",
      "\n",
      "Most Similar User 1 : User_Id 33489  with distance of  0.3112653467609411\n",
      "Most Similar User 2 : User_Id 40608  with distance of  0.3682205787628192\n",
      "Most Similar User 3 : User_Id 33869  with distance of  0.3892624325108006\n",
      "Most Similar User 4 : User_Id 37611  with distance of  0.38990171750167435\n",
      "Most Similar User 5 : User_Id 22240  with distance of  0.39371444479174234\n",
      "Most Similar User 6 : User_Id 41709  with distance of  0.40300590935821234\n",
      "Most Similar User 7 : User_Id 14573  with distance of  0.4031589767307432\n",
      "Most Similar User 8 : User_Id 49621  with distance of  0.40539945329571436\n",
      "Most Similar User 9 : User_Id 1584  with distance of  0.40924949954285794\n",
      "Most Similar User 10 : User_Id 33452  with distance of  0.40971315104147477\n",
      "All the jokes rated  by 10  similar users are all  rated by the selected user. Similar user count  is increased by 5 to  16\n",
      "Similar Users for User 38830  \n",
      "\n",
      "Most Similar User 1 : User_Id 33489  with distance of  0.3112653467609411\n",
      "Most Similar User 2 : User_Id 40608  with distance of  0.3682205787628192\n",
      "Most Similar User 3 : User_Id 33869  with distance of  0.3892624325108006\n",
      "Most Similar User 4 : User_Id 37611  with distance of  0.38990171750167435\n",
      "Most Similar User 5 : User_Id 22240  with distance of  0.39371444479174234\n",
      "Most Similar User 6 : User_Id 41709  with distance of  0.40300590935821234\n",
      "Most Similar User 7 : User_Id 14573  with distance of  0.4031589767307432\n",
      "Most Similar User 8 : User_Id 49621  with distance of  0.40539945329571436\n",
      "Most Similar User 9 : User_Id 1584  with distance of  0.40924949954285794\n",
      "Most Similar User 10 : User_Id 33452  with distance of  0.40971315104147477\n",
      "Most Similar User 11 : User_Id 33729  with distance of  0.4144614446186403\n",
      "Most Similar User 12 : User_Id 37937  with distance of  0.4185329263632104\n",
      "Most Similar User 13 : User_Id 35488  with distance of  0.4188680272824561\n",
      "Most Similar User 14 : User_Id 19143  with distance of  0.4211445312274211\n",
      "Most Similar User 15 : User_Id 33394  with distance of  0.4211592675159156\n",
      "All the jokes rated  by 15  similar users are all  rated by the selected user. Similar user count  is increased by 5 to  21\n",
      "Similar Users for User 38830  \n",
      "\n",
      "Most Similar User 1 : User_Id 33489  with distance of  0.3112653467609411\n",
      "Most Similar User 2 : User_Id 40608  with distance of  0.3682205787628192\n",
      "Most Similar User 3 : User_Id 33869  with distance of  0.3892624325108006\n",
      "Most Similar User 4 : User_Id 37611  with distance of  0.38990171750167435\n",
      "Most Similar User 5 : User_Id 22240  with distance of  0.39371444479174234\n",
      "Most Similar User 6 : User_Id 41709  with distance of  0.40300590935821234\n",
      "Most Similar User 7 : User_Id 14573  with distance of  0.4031589767307432\n",
      "Most Similar User 8 : User_Id 49621  with distance of  0.40539945329571436\n",
      "Most Similar User 9 : User_Id 1584  with distance of  0.40924949954285794\n",
      "Most Similar User 10 : User_Id 33452  with distance of  0.40971315104147477\n",
      "Most Similar User 11 : User_Id 33729  with distance of  0.4144614446186403\n",
      "Most Similar User 12 : User_Id 37937  with distance of  0.4185329263632104\n",
      "Most Similar User 13 : User_Id 35488  with distance of  0.4188680272824561\n",
      "Most Similar User 14 : User_Id 19143  with distance of  0.4211445312274211\n",
      "Most Similar User 15 : User_Id 33394  with distance of  0.4211592675159156\n",
      "Most Similar User 16 : User_Id 45437  with distance of  0.42435492375845196\n",
      "Most Similar User 17 : User_Id 19112  with distance of  0.4243907755317925\n",
      "Most Similar User 18 : User_Id 39099  with distance of  0.4261089287243204\n",
      "Most Similar User 19 : User_Id 23569  with distance of  0.4263523318643928\n",
      "Most Similar User 20 : User_Id 47104  with distance of  0.42973622836737646\n",
      "All the jokes rated  by 20  similar users are all  rated by the selected user. Similar user count  is increased by 5 to  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2482876684.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2482876684.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2482876684.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2482876684.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Users for User 38830  \n",
      "\n",
      "Most Similar User 1 : User_Id 33489  with distance of  0.3112653467609411\n",
      "Most Similar User 2 : User_Id 40608  with distance of  0.3682205787628192\n",
      "Most Similar User 3 : User_Id 33869  with distance of  0.3892624325108006\n",
      "Most Similar User 4 : User_Id 37611  with distance of  0.38990171750167435\n",
      "Most Similar User 5 : User_Id 22240  with distance of  0.39371444479174234\n",
      "Most Similar User 6 : User_Id 41709  with distance of  0.40300590935821234\n",
      "Most Similar User 7 : User_Id 14573  with distance of  0.4031589767307432\n",
      "Most Similar User 8 : User_Id 49621  with distance of  0.40539945329571436\n",
      "Most Similar User 9 : User_Id 1584  with distance of  0.40924949954285794\n",
      "Most Similar User 10 : User_Id 33452  with distance of  0.40971315104147477\n",
      "Most Similar User 11 : User_Id 33729  with distance of  0.4144614446186403\n",
      "Most Similar User 12 : User_Id 37937  with distance of  0.4185329263632104\n",
      "Most Similar User 13 : User_Id 35488  with distance of  0.4188680272824561\n",
      "Most Similar User 14 : User_Id 19143  with distance of  0.4211445312274211\n",
      "Most Similar User 15 : User_Id 33394  with distance of  0.4211592675159156\n",
      "Most Similar User 16 : User_Id 45437  with distance of  0.42435492375845196\n",
      "Most Similar User 17 : User_Id 19112  with distance of  0.4243907755317925\n",
      "Most Similar User 18 : User_Id 39099  with distance of  0.4261089287243204\n",
      "Most Similar User 19 : User_Id 23569  with distance of  0.4263523318643928\n",
      "Most Similar User 20 : User_Id 47104  with distance of  0.42973622836737646\n",
      "Most Similar User 21 : User_Id 45268  with distance of  0.43215371511823464\n",
      "Most Similar User 22 : User_Id 14150  with distance of  0.43236000056254287\n",
      "Most Similar User 23 : User_Id 39895  with distance of  0.4332028427228859\n",
      "Most Similar User 24 : User_Id 45191  with distance of  0.4333682258163196\n",
      "Most Similar User 25 : User_Id 22602  with distance of  0.4356633815101508\n",
      "All the jokes rated  by 25  similar users are all  rated by the selected user. Similar user count  is increased by 5 to  31\n",
      "Similar Users for User 38830  \n",
      "\n",
      "Most Similar User 1 : User_Id 33489  with distance of  0.3112653467609411\n",
      "Most Similar User 2 : User_Id 40608  with distance of  0.3682205787628192\n",
      "Most Similar User 3 : User_Id 33869  with distance of  0.3892624325108006\n",
      "Most Similar User 4 : User_Id 37611  with distance of  0.38990171750167435\n",
      "Most Similar User 5 : User_Id 22240  with distance of  0.39371444479174234\n",
      "Most Similar User 6 : User_Id 41709  with distance of  0.40300590935821234\n",
      "Most Similar User 7 : User_Id 14573  with distance of  0.4031589767307432\n",
      "Most Similar User 8 : User_Id 49621  with distance of  0.40539945329571436\n",
      "Most Similar User 9 : User_Id 1584  with distance of  0.40924949954285794\n",
      "Most Similar User 10 : User_Id 33452  with distance of  0.40971315104147477\n",
      "Most Similar User 11 : User_Id 33729  with distance of  0.4144614446186403\n",
      "Most Similar User 12 : User_Id 37937  with distance of  0.4185329263632104\n",
      "Most Similar User 13 : User_Id 35488  with distance of  0.4188680272824561\n",
      "Most Similar User 14 : User_Id 19143  with distance of  0.4211445312274211\n",
      "Most Similar User 15 : User_Id 33394  with distance of  0.4211592675159156\n",
      "Most Similar User 16 : User_Id 45437  with distance of  0.42435492375845196\n",
      "Most Similar User 17 : User_Id 19112  with distance of  0.4243907755317925\n",
      "Most Similar User 18 : User_Id 39099  with distance of  0.4261089287243204\n",
      "Most Similar User 19 : User_Id 23569  with distance of  0.4263523318643928\n",
      "Most Similar User 20 : User_Id 47104  with distance of  0.42973622836737646\n",
      "Most Similar User 21 : User_Id 45268  with distance of  0.43215371511823464\n",
      "Most Similar User 22 : User_Id 14150  with distance of  0.43236000056254287\n",
      "Most Similar User 23 : User_Id 39895  with distance of  0.4332028427228859\n",
      "Most Similar User 24 : User_Id 45191  with distance of  0.4333682258163196\n",
      "Most Similar User 25 : User_Id 22602  with distance of  0.4356633815101508\n",
      "Most Similar User 26 : User_Id 44643  with distance of  0.4405115444563583\n",
      "Most Similar User 27 : User_Id 19004  with distance of  0.4410303016889868\n",
      "Most Similar User 28 : User_Id 39664  with distance of  0.4411036359619984\n",
      "Most Similar User 29 : User_Id 29292  with distance of  0.44128980554626573\n",
      "Most Similar User 30 : User_Id 38462  with distance of  0.4424026695883603\n",
      "All the jokes rated  by 30  similar users are all  rated by the selected user. Similar user count  is increased by 5 to  36\n",
      "Similar Users for User 38830  \n",
      "\n",
      "Most Similar User 1 : User_Id 33489  with distance of  0.3112653467609411\n",
      "Most Similar User 2 : User_Id 40608  with distance of  0.3682205787628192\n",
      "Most Similar User 3 : User_Id 33869  with distance of  0.3892624325108006\n",
      "Most Similar User 4 : User_Id 37611  with distance of  0.38990171750167435\n",
      "Most Similar User 5 : User_Id 22240  with distance of  0.39371444479174234\n",
      "Most Similar User 6 : User_Id 41709  with distance of  0.40300590935821234\n",
      "Most Similar User 7 : User_Id 14573  with distance of  0.4031589767307432\n",
      "Most Similar User 8 : User_Id 49621  with distance of  0.40539945329571436\n",
      "Most Similar User 9 : User_Id 1584  with distance of  0.40924949954285794\n",
      "Most Similar User 10 : User_Id 33452  with distance of  0.40971315104147477\n",
      "Most Similar User 11 : User_Id 33729  with distance of  0.4144614446186403\n",
      "Most Similar User 12 : User_Id 37937  with distance of  0.4185329263632104\n",
      "Most Similar User 13 : User_Id 35488  with distance of  0.4188680272824561\n",
      "Most Similar User 14 : User_Id 19143  with distance of  0.4211445312274211\n",
      "Most Similar User 15 : User_Id 33394  with distance of  0.4211592675159156\n",
      "Most Similar User 16 : User_Id 45437  with distance of  0.42435492375845196\n",
      "Most Similar User 17 : User_Id 19112  with distance of  0.4243907755317925\n",
      "Most Similar User 18 : User_Id 39099  with distance of  0.4261089287243204\n",
      "Most Similar User 19 : User_Id 23569  with distance of  0.4263523318643928\n",
      "Most Similar User 20 : User_Id 47104  with distance of  0.42973622836737646\n",
      "Most Similar User 21 : User_Id 45268  with distance of  0.43215371511823464\n",
      "Most Similar User 22 : User_Id 14150  with distance of  0.43236000056254287\n",
      "Most Similar User 23 : User_Id 39895  with distance of  0.4332028427228859\n",
      "Most Similar User 24 : User_Id 45191  with distance of  0.4333682258163196\n",
      "Most Similar User 25 : User_Id 22602  with distance of  0.4356633815101508\n",
      "Most Similar User 26 : User_Id 44643  with distance of  0.4405115444563583\n",
      "Most Similar User 27 : User_Id 19004  with distance of  0.4410303016889868\n",
      "Most Similar User 28 : User_Id 39664  with distance of  0.4411036359619984\n",
      "Most Similar User 29 : User_Id 29292  with distance of  0.44128980554626573\n",
      "Most Similar User 30 : User_Id 38462  with distance of  0.4424026695883603\n",
      "Most Similar User 31 : User_Id 5024  with distance of  0.442549984696765\n",
      "Most Similar User 32 : User_Id 49293  with distance of  0.4436320815481136\n",
      "Most Similar User 33 : User_Id 46  with distance of  0.4443836598965859\n",
      "Most Similar User 34 : User_Id 37252  with distance of  0.4460392505230518\n",
      "Most Similar User 35 : User_Id 33834  with distance of  0.4472292081360385\n",
      "\n",
      " \n",
      " For User  38830 recommended joke is: \n",
      " Hillary, Bill Clinton and the Pope are sitting together on an airplane.\n",
      "\n",
      "Bill says \"I could throw one thousand dollar bill out of this plane and\n",
      "make one person very happy.\"\n",
      "\n",
      "Hillary says \"I could throw 10 hundred dollar bills out of the plane and\n",
      "make 10 people very happy.\"\n",
      "\n",
      "The Pope chips in and says \"I could throw Bill out of the airplane and make the whole \n",
      "country happy.\"\n",
      "  \n",
      "  with Rating by Similar Users:  9.969  \n",
      "  with General Rating:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2482876684.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2482876684.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n"
     ]
    }
   ],
   "source": [
    "# Getting the  list of unique user IDs in training data\n",
    "user_ids = train_data['userId'].unique()\n",
    "user_ids.sort()\n",
    "user_ids=user_ids.tolist()\n",
    "\n",
    "#Getting the Joke Recommandation based on KNN to a Random User from the list of User IDs \n",
    "get_KNN_recommandations_for_user(np.random.choice(user_ids))  #Input user id can be changed to any user id available in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Showing each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input users index is  5059\n",
      " \n",
      " distances are \n",
      "[3.33066907e-16 3.95359561e-01 4.34080177e-01 4.37761506e-01\n",
      " 4.73041320e-01 4.76263873e-01 4.79189974e-01 4.80054261e-01\n",
      " 4.86133902e-01 4.86884173e-01 4.89175830e-01]\n",
      " \n",
      " indices are\n",
      "[5059 2418 3732 2404 8043 6381 4110 2504 6439 7770 6621]\n",
      " \n",
      " Similar Users for User 35560  \n",
      "\n",
      "Most Similar User 1 : User_Id 16481  with distance of  0.39535956062392763\n",
      "Most Similar User 2 : User_Id 29387  with distance of  0.43408017655388254\n",
      "Most Similar User 3 : User_Id 16335  with distance of  0.4377615061779373\n",
      "Most Similar User 4 : User_Id 48727  with distance of  0.4730413198706176\n",
      "Most Similar User 5 : User_Id 41091  with distance of  0.47626387279734816\n",
      "Most Similar User 6 : User_Id 31271  with distance of  0.47918997439820976\n",
      "Most Similar User 7 : User_Id 17166  with distance of  0.48005426069764656\n",
      "Most Similar User 8 : User_Id 41332  with distance of  0.4861339017089066\n",
      "Most Similar User 9 : User_Id 47490  with distance of  0.4868841725748124\n",
      "Most Similar User 10 : User_Id 42204  with distance of  0.48917583040509516\n",
      " \n",
      " similaruserid list \n",
      "[16481, 29387, 16335, 48727, 41091, 31271, 17166, 41332, 47490, 42204]\n",
      " \n",
      " similar_user_data \n",
      "         userId  jokeId  rating  \\\n",
      "420204    16335       7   5.250   \n",
      "420205    16335       8  -0.188   \n",
      "420206    16335      13   5.219   \n",
      "420207    16335      15   6.938   \n",
      "420208    16335      16   2.156   \n",
      "...         ...     ...     ...   \n",
      "1370155   48727     146   3.969   \n",
      "1370101   48727     147   6.969   \n",
      "1370148   48727     148   4.062   \n",
      "1370061   48727     149   3.750   \n",
      "1370056   48727     150   3.500   \n",
      "\n",
      "                                                  jokeText  \\\n",
      "420204   How many feminists does it take to screw in a ...   \n",
      "420205   Q. Did you hear about the dyslexic devil worsh...   \n",
      "420206   They asked the Japanese visitor if they have e...   \n",
      "420207   Q:  What did the blind person say when given s...   \n",
      "420208   Q. What is orange and sounds like a parrot?  \\...   \n",
      "...                                                    ...   \n",
      "1370155  America: 8:00 - Welcome to work! 12:00 - Lunch...   \n",
      "1370101  It was the day of the big sale. Rumors of the ...   \n",
      "1370148  Recently a teacher, a garbage collector, and a...   \n",
      "1370061  A little girl asked her father, \"Daddy? Do all...   \n",
      "1370056  In an interview with David Letterman, Carter p...   \n",
      "\n",
      "         avg_rating_by_similar_users  \n",
      "420204                      0.428100  \n",
      "420205                      4.021900  \n",
      "420206                      4.881100  \n",
      "420207                      4.050000  \n",
      "420208                     -0.153100  \n",
      "...                              ...  \n",
      "1370155                     1.400833  \n",
      "1370101                     5.987400  \n",
      "1370148                     4.046750  \n",
      "1370061                     4.385667  \n",
      "1370056                     4.854111  \n",
      "\n",
      "[912 rows x 5 columns]\n",
      " \n",
      " JokesfromSimilarUsers \n",
      "         jokeId                                           jokeText  \\\n",
      "420204        7  How many feminists does it take to screw in a ...   \n",
      "420205        8  Q. Did you hear about the dyslexic devil worsh...   \n",
      "420206       13  They asked the Japanese visitor if they have e...   \n",
      "420207       15  Q:  What did the blind person say when given s...   \n",
      "420208       16  Q. What is orange and sounds like a parrot?  \\...   \n",
      "...         ...                                                ...   \n",
      "1148275     123  When most people claim to be \"killing time\", i...   \n",
      "1148251     124  Person 1: Hey, wanna hear a great knock-knock ...   \n",
      "1148274     140  Chuck Norris' calendar goes straight from Marc...   \n",
      "1148276     141  Jack Bauer can get McDonald's breakfast after ...   \n",
      "1148171     149  A little girl asked her father, \"Daddy? Do all...   \n",
      "\n",
      "         avg_rating_by_similar_users  \n",
      "420204                      0.428100  \n",
      "420205                      4.021900  \n",
      "420206                      4.881100  \n",
      "420207                      4.050000  \n",
      "420208                     -0.153100  \n",
      "...                              ...  \n",
      "1148275                     3.354000  \n",
      "1148251                     3.854333  \n",
      "1148274                     3.708333  \n",
      "1148276                     1.604000  \n",
      "1148171                     4.385667  \n",
      "\n",
      "[128 rows x 3 columns]\n",
      " \n",
      " JokesRatedBySelectedUser \n",
      "[\"How many feminists does it take to screw in a light bulb?\\nThat's not funny.\\n\", 'Q. Did you hear about the dyslexic devil worshiper? \\n\\nA. He sold his soul to Santa.\\n', 'They asked the Japanese visitor if they have elections in his\\ncountry.  \\n\"Every Morning\" he answers.\\n', 'Q:  What did the blind person say when given some matzah?\\n\\nA:  Who the hell wrote this?\\n', 'Q. What is orange and sounds like a parrot?  \\n\\nA. A carrot.\\n', 'How many men does it take to screw in a light bulb? \\n\\nOne...men will screw anything. \\n', 'A dog walks into Western Union and asks the clerk to send a telegram. He fills out a form on which he\\nwrites down the telegram he wishes to send: \"Bow wow wow, Bow wow wow.\"\\n\\nThe clerk says, \"You can add another \\'Bow wow\\' for the same price.\"\\n\\nThe dog responded, \"Now wouldn\\'t that sound a little silly?\" \\n', 'Q: If a person who speaks three languages is called \"tri-lingual,\" and\\na person who speaks two languages is called \"bi-lingual,\" what do call\\na person who only speaks one language?\\n\\nA: American! \\n', 'A duck walks into a pharmacy and asks for a condom. The pharmacist says\\n\"Would you like me to stick that on your bill?\"\\nThe duck says: \\n\"What kind of duck do you think I am!\"\\n', \"Q: What is the Australian word for a boomerang that won't\\n   come back? \\n\\nA: A stick\\n\", 'Two kindergarten girls were talking outside: one said, \"You\\nwon\\'t believe what I saw on the patio yesterday--a condom!\"\\n\\nThe second girl asked, \"What\\'s a patio?\"\\n', 'A mechanical, electrical and a software engineer from Microsoft were\\ndriving through the desert when the car broke down. The mechanical\\nengineer said \"It seems to be a problem with the fuel injection system,\\nwhy don\\'t we pop the hood and I\\'ll take a look at it.\" To which the\\nelectrical engineer replied, \"No I think it\\'s just a loose ground wire,\\nI\\'ll get out and take a look.\" Then, the Microsoft engineer jumps in.\\n\"No, no, no. If we just close up all the windows, get out, wait a few\\nminutes, get back in, and then reopen the windows everything will work\\nfine.\"\\n', 'An old Scotsmen is sitting with a younger Scottish gentleman and says the boy. \\n\"Ah, lad look out that window. You see that stone wall there, I built it with\\nme own bare hands, placed every stone meself.  But do they call me MacGregor the\\nwall builder? No! \\n\\nHe Takes a few sips of his beer then says, \"Aye, and look out on that lake and \\neye that beautiful pier. I built it meself, laid every board and hammered each\\nnail but do they call me MacGregor the pier builder? No! \\n\\nHe continues...\"And lad, you see that road? That too I build with me own bare \\nhands. Laid every inch of pavement meself, but do they call MacGregor the road\\nbuilder? No!\"\\n\\nAgain he returns to his beer for a few sips, then says, \\n\"Agh, but you screw one sheep...\"\\n', 'What do you call an American in the finals of the world cup?\\n\\n\"Hey Beer Man!\"\\n', 'Out in the backwoods of some midwestern state, little Johnny arrives\\nat school an hour late.\\n\\nTeacher: \"Why are you so late, John?\"\\nJohny : \"My big brother got shot in the ass.\"\\n(the teacher corrects his speech)\\nTeacher: \"Rectum.\"\\nJohnny : \"Wrecked him!? Hell, It damn near killed him!\" \\n', 'A Jewish young man was seeing a psychiatrist for an eating and\\nsleeping disorder. \\n\\n\"I am so obsessed with my mother... As soon as I go to sleep, I start\\ndreaming, and everyone in my dream turns into my mother. I wake up in\\nsuch a state, all I can do is go downstairs and eat a piece of toast.\"\\n\\nThe psychiatrist replies:\\n\\n\"What, just one piece of toast, for a big boy like you?\"\\n', '\"May I take your order?\" the waiter asked. \\n\\n\"Yes, how do you prepare your chickens?\" \\n\\n\"Nothing special sir,\" he replied. \"We just tell them straight out\\nthat they\\'re going to die.\"\\n', 'What is the difference between men and women:\\n\\n\\nA woman wants one man to satisfy her every need.\\nA man wants every woman to satisfy his one need.\\n', 'How many Irishmen does it take to change a lightbulb?\\n\\nTwo, one to hold the lightbulb and the other to drink until the room spins. \\n', 'What does an atheist say during an orgasm?\\n\"Oh Darwin! Oh Darwin!...\"\\n', 'Two men are discussing the age old question: who enjoys sex more, the\\nman or the woman?  A woman walks by and listens in for awhile and then\\ninterrupts: \\n\"Listen you guys. You know when your ear itches and you put in your \\nlittle finger and wiggle it around for awhile?  Afterward,\\nwhich feels better, your finger or your ear?\"\\n', 'A boy comes home from school and tells his mother that he got a part\\nin the school play.  \"What part?\" the mother asked. \"I play a Jewish\\nhusband,\" the boy replied.  \\n\"Go back to school and tell your teacher that you want a speaking role!\"\\n', 'A couple has been married for 75 years. For the husband\\'s 95th\\nbirthday, his wife decides to surprise him by hiring a prostitute.\\nThat day, the doorbell rings. The husband uses his walker to get to\\nthe door and opens it. \\nA 21-year-old in a latex outfit smiles and\\nsays, \"Hi, I here to give you super sex!\" \\nThe old man says, \"I\\'ll take the soup.\"\\n', 'The graduate with a Science degree asks, \"Why does it work?\"\\nThe graduate with an Engineering degree asks, \"How does it work?\"\\nThe graduate with an Accounting degree Asks, \"How much will it cost?\" \\nThe graduate with a  Liberal Arts degree asks, \"Do you want fries \\nwith  that?\"\\n', 'Three engineering students were gathered together discussing the\\npossible designers of the human body.  \\nOne said, \"It was a mechanical engineer. Just look at all the joints.\"  \\nAnother said, \"No, it was an electrical engineer.  The nervous systems many thousands of electrical\\nconnections.\"  \\nThe last said, \"Actually it was a civil engineer. Who else would run a toxic waste pipeline through a recreational area?\"\\n', 'A guy goes into confession and says to the priest, \"Father, I\\'m 80 years\\nold, widower, with 11 grandchildren. Last night I met two beautiful flight\\nattendants. They took me home and I made love to both of them. Twice.\"\\n\\nThe priest said: \"Well, my son, when was the last time you were in\\nconfession?\"\\n \"Never Father, I\\'m Jewish.\"\\n \"So then, why are you telling me?\"\\n \"I\\'m telling everybody.\"\\n', 'A woman has twins, and gives them up for adoption.  One of\\nthem goes to a family in Egypt and is named \"Amal.\"  The other goes to\\na  family in Spain; they name him \"Juan.\"  Years later, Juan sends a\\npicture of himself to his mom.  Upon receiving the picture, she tells\\nher husband that she wishes she also had a picture of Amal.  \\nHer husband responds, \"But they are twins-if you\\'ve seen Juan, you\\'ve\\nseen   Amal.\\n', 'A man and Cindy Crawford get stranded on a desert island.  After a couple\\nof days they fall in love and start sleeping together.  Time pass the\\nman seems frustrated, Cindy asks if there is anything she can do?  He\\nsays there is one thing, \"Could you put on this baseball cap and go to\\nthe other side of the island and answer me when I call you Bob?\"  She\\nagrees.  Next day he is walking on the other side of the island, runs\\ninto her and says \"Hi Bob!\"  \\nShe says \"Hello, what\\'s up?\"  \\nHe replies: \"Bob you won\\'t believe it: I\\'ve been sleeping with Cindy\\nCrawford for the past two weeks!!!!\"\\n', 'What did the Buddhist say to the hot dog vendor?\\nMake me one with everything.\\n', 'A group of  managers were given the assignment to measure the\\nheight of a flagpole. So they go out to the flagpole with ladders\\nand tape measures, and they\\'re falling off the ladders, dropping\\nthe tape measures - the whole thing is just a mess.\\nAn engineer comes along and sees what they\\'re trying to do,\\nwalks over, pulls the flagpole out of the ground, lays it flat,\\nmeasures it from end to end, gives the measurement to one of the\\nmanagers and walks away.\\nAfter the engineer has gone, one manager turns to another and\\nlaughs. \"Isn\\'t that just like an engineer, we\\'re looking for the \\nheight and he gives us the length.\"\\n', 'An engineer, a physicist and a mathematician are sleeping in a\\nroom. There is a fire in the room. The engineer wakes up, sees the fire,\\npicks up the bucket of water and douses the fire and goes back to\\nsleep. \\n\\nAgain there is fire in the room. This time, the physicist wakes\\nup, notices the bucket, fills it with water, calculates the optimal\\ntrajectory and douses the fire in minimum amount of water and goes\\nback to sleep. \\n\\nAgain there is fire. This time the mathematician wakes up. \\nHe looks at the fire, looks at the bucket and the water and\\nexclaims, \"A solution exists\" and goes back to sleep.\\n', 'Once upon a time, two brooms fell in love and decided to get married.\\nBefore the ceremony, the bride broom informed the groom broom that \\nshe was expecting a little whiskbroom. The groom broom was aghast!\\n\\n\"How is this possible?\" he asked. \"We\\'ve never swept together!\\n', 'A man piloting a hot air balloon discovers he has wandered off course and\\nis hopelessly lost. He descends to a lower altitude and locates a man\\ndown on the ground. He lowers the balloon further and shouts \"Excuse me,\\ncan you tell me where I am?\"\\n\\nThe man below says: \"Yes, you\\'re in a hot air balloon, about 30 feet\\nabove this field.\"\\n\\n\"You must work in Information Technology,\" says the balloonist.\\n\\n\"Yes I do,\" replies the man. \"And how did you know that?\"\\n\\n\"Well,\" says the balloonist, \"what you told me is technically correct,\\nbut of no use to anyone.\"\\n\\nThe man below says, \"You must work in management.\"\\n\\n\"I do,\" replies the balloonist, \"how did you know?\"\\n\\n\"Well,\" says the man, \"you don\\'t know where you are, or where you\\'re\\ngoing, but you expect my immediate help. You\\'re in the same position you\\nwere before we met, but now it\\'s my fault!\"\\n\\n', 'Employer to applicant: \"In this job we need someone who is responsible.\"\\n\\nApplicant: \"I\\'m the one you want. On my last job, every time anything\\nwent wrong, they said I was responsible.\"\\n\\n', 'At a recent Sacramento PC Users Group meeting,\\na company was demonstrating its latest speech-\\nrecognition software.   A representative from the\\ncompany was just about ready to start the\\ndemonstration and asked everyone in the room\\nto quiet down.\\n\\nJust then someone in the back of the room yelled,\\n\"Format C: Return.\"\\n\\nSomeone else chimed in:\\n\"Yes, Return\"\\n\\nUnfortunately, the software worked.\\n\\n', 'Q: How many stalkers does it take to change a light bulb?\\n\\nA: Two. One to replace the bulb, and the other to watch it day and night.\\n', 'Q: Do you know the difference between an intelligent male and the\\nSasquatch?\\n\\nA: There have been actual reported sightings of the Sasquatch.\\n', \"Q: What's the difference between the government  and  the Mafia?\\n\\nA: One of them is organized.\\n\", 'Q: How do you keep a computer programmer in the \\nshower all day long?\\n\\nA: Give them a shampoo with a label that says\\n\"rinse, lather, repeat\".\\n', 'A neutron walks into a bar and orders a drink.\\n\"How much do I owe you?\" the neutron asks.\\n\\nThe bartender replies, \"for you, no charge.\"\\n', 'A Panda bear walks into a bar.  Sits down at a table and orders a beer \\nand a double cheeseburger.  After he is finished eating, he pulls out a gun\\nand rips the place with gunfire.  Patrons scatter and dive under chairs and\\ntables as the bear runs out the door.  After ensuring that no one is hurt, \\nthe bartender races out the door, and calls after the bear \"What the hell did\\nyou do that for?\"  The bear calls back, \"I\\'m a Panda bear.  Look it up in the\\ndictionary.\"  \\n\\nThe bartender returns, pulls out his dictionary.\\n\\npanda : \\\\Pan\"da\\\\, n. (Zo[\"o]l.)\\nA small Asiatic mammal (Ailurus fulgens) having fine soft fur.\\nIt is related to the bears, and inhabits the mountains of Northern India.\\nEats shoots and leaves.\\n', 'Early one morning a mother went to her sleeping son and woke him up.\\n\\n\"Wake up, son.  It\\'s time to go to school.\" \\n\"But why, Mama?  I don\\'t want to go to school.\" \\n\"Give me two reasons why you don\\'t want to go to school.\" \\n\"One, all the children hate me. Two, all the teachers hate me,\" \\n\"Oh! that\\'s no reason. Come on, you have to go to school,\" \\n\\n\"Give me two good reasons WHY I should go to school?\" \\n \\n\"One, you are fifty-two years old. Two, you are the principal of the\\n school.\"\\n ', 'Reaching the end of a job interview, the human resources person asked a\\nyoung engineer fresh out of Stanford,\\n\\n\"And what starting salary were you looking for?\"\\n\\nThe engineer said, \"In the neighborhood of $125,000 a year, depending\\non the benefits package.\"\\n\\nThe interviewer said, \"Well, what would you say to a package of 5-weeks \\nvacation, 14 paid holidays, full medical and dental, company matching \\nretirement fund to 50% of salary, and a company car leased every 2 years - \\nsay, a red Corvette?\"\\n\\nThe Engineer sat up straight and said, \"Wow! Are you kidding?\"\\n\\nAnd the interviewer replied, \"Yeah, but you started it.\"\\n', 'Two atoms are walking down the street when one \\natom says to the other \\n\"Oh, my! I\\'ve lost an electron!\"\\n\\nThe second atom says\"Are you sure\"\\n\\nThe first replies \"I\\'m positive!\"\\n', 'Just a thought ..\\n\\nBefore criticizing someone, walk a mile in their shoes.  \\n\\nThen when you do criticize them, \\nyou will be a mile away and have their shoes !\\n', 'Two attorneys went into a diner and ordered two drinks.  Then they produced \\nsandwiches from their briefcases and started to eat.  The owner became\\nquite concerned and marched over and told them, \"You can\\'t eat your own\\nsandwiches in here!\"\\n\\nThe attorneys looked at each other, shrugged their shoulders and then\\nexchanged sandwiches.\\n', 'A teacher is explaining to her class how different languages use \\nnegatives differently.  She says, \"In all languages, a positive followed\\nby a negative or a negative followed by a positive makes a negative.  In\\nsome languages, two negatives together make a positive, while in others they\\nmake a negative.  But in no language do two positives make a negative.\"  \\n\\nOne of the students puts up his hand and says, \"Yeah, right.\"\\n', 'A bus station is where a bus stops.\\nA train station is where a train stops.\\n\\nOn my desk I have a work station...\\n', \"Nurse: Doctor, Doctor, there's an invisible man in the waiting room! Doctor: Well, go in there and tell him that I can't see him!\", 'As a pre-med student, I had to take a difficult class in physics. One day our professor was discussing a particularly complicated concept. A student rudely interrupted to ask, \"Why do we have to learn this stuff?\" \"To save lives.\" The professor responded quickly and continued the lecture. A few minutes later, the same student spoke up again. \"So how does physics save lives?\" he persisted. \"It usually keeps the idiots like you out of medical school,\" replied the professor.', \"(A) The Japanese eat very little fat and suffer fewer heart attacks than the British or Americans. (B) On the other hand, the French eat a lot of fat and also suffer fewer heart attacks than the British or Americans. (C) The Chinese drink very little red wine and suffer fewer heart attacks than the British or Americans. (D) The Italians drink excessive amounts of red wine and also suffer fewer heart attacks than the British or Americans. (E) Conclusion: Eat and drink what you like. It's speaking English that kills you.\", 'A man approached a very beautiful woman in a large supermarket and asked, \"You know, I\\'ve lost my wife here in the supermarket. Can you talk to me for a couple of minutes?\" \"Why?\" \"Because every time I talk to a beautiful woman my wife appears out of nowhere.\"', 'Do you believe in life after death? the boss asked one of his employees. \"Yes, sir,\" the new recruit replied. \"Well, then, that makes everything just fine...\" the boss went on. \"After you left early yesterday to go to your grandmother\\'s funeral, she stopped in to see you.\"', 'One day, a professor was giving a big test to his students. He handed out the tests and went back to his desk to wait. Once the test was over, the students all handed the tests back in. The professor noticed that one of the students had attached a $100 bill to his test with a note saying: \"A dollar per point.\" The next class the professor handed the tests back out. This student got back his test...and $64 change!', 'A new business was opening and one of the owner\\'s friends wanted to send him flowers for the occasion. They arrived at the new business site and the owner read the card, \"Rest in Peace.\" The owner was angry and called the florist to complain. After he had told the florist of the obvious mistake and how angry he was, the florist replied, \"Sir, I\\'m really sorry for the mistake, but rather than getting angry, you should imagine this: somewhere there is a funeral taking place today, and they have flowers with a note saying, \\'Congratulations on your new location!\\'\"', 'When my three-year-old son opened the birthday gift from his grandmother, he discovered a water pistol. He squealed with delight and headed for the nearest sink. I was not so pleased. I turned to Mom and said, \"I\\'m surprised at you. Don\\'t you remember how we used to drive you crazy with water guns?\" Mom smiled and then replied...\"I remember.\"', 'The new employee stood before the paper shredder looking confused. \"Need some help?\" a secretary asked. \"Yes,\" he replied. \"How does this thing work?\" \"Simple,\" she said, taking the fat report from his hand and feeding it into the shredder. \"Thanks, but where do the copies come out?\"', 'A man goes into a drug store and asks the pharmacist if he can give him something for the hiccups. The pharmacist promptly reaches out and slaps the man\\'s face. \"What the heck did you do that for?!\" the man screams. \"Well, you don\\'t have the hiccups anymore, do you?\" The man says, \"No I don\\'t, you IDIOT...but my wife out in the car still does!\"', 'One day the first grade teacher was reading the story of the Three Little Pigs to her class. She came to the part of the story where the first pig was trying to accumulate the building materials for his home. She read, \"...and so the pig went up to the man with the wheelbarrow full of straw and said, \\'Pardon me sir, but may I have some of that straw to build my house?\\'\" The teacher paused then asked the class, \"And what do you think that man said?\" One little boy raised his hand and said, \"I know...he said, \\'Holy Shit! A talking pig!\\'\"', 'A drunk staggers into a Catholic Church, enters a confessional booth, sits down, but says nothing. The Priest coughs a few times to get his attention but the drunk just sits there. Finally, the Priest pounds three times on the wall. The drunk mumbles, \"Ain\\'t no use knockin, there\\'s no paper on this side either.\"', 'An astronomer, a physicist and a mathematician (it is said) were holidaying in Scotland. Glancing from a train window, they observed a black sheep in the middle of a field. \"How interesting,\" observed the astronomer, \"All Scottish sheep are black!\" To which the physicist responded, \"No, no! Some Scottish sheep are black!\" The mathematician gazed heavenward in supplication, and then intoned, \"In Scotland there exists at least one field, containing at least one sheep, at least one side of which is black.\"', 'A Briton, a Frenchman and a Russian are viewing a painting of Adam and Eve frolicking in the Garden of Eden. \"Look at their reserve, their calm,\" muses the Brit. \"They must be British.\" \"Nonsense,\" the Frenchman disagrees. \"They\\'re naked, and so beautiful. Clearly, they are French.\" \"No way! They have no clothes and no shelter,\" the Russian points out, \"They have only an apple to eat, and they are being told they live in a paradise. Obviously, they are Russian.\"', 'An old man goes to the doctor for his yearly physical, his wife tagging along. When the doctor enters the examination room, he tells the old man, \"I need a urine sample, a stool sample and a sperm sample.\" The old man, being hard of hearing, looks at his wife and yells: \"WHAT? What did he say? What\\'s he want?\" His wife yells back, \"He needs your underwear.\"', 'A guy had been feeling down for so long that he finally decided to seek the aid of a psychiatrist. He went there, lay on the couch, spilled his guts then waited for the profound wisdom of the psychiatrist to make him feel better. The psychiatrist asked me a few questions, took some notes then sat thinking in silence for a few minutes with a puzzled look on his face. He looked up with an expression of delight and said, \"I think your problem is low self-esteem. It is very common among losers.\"', 'The new employee stood before the paper shredder looking confused. \"Need some help?\" a secretary, walking by, asked. \"Yes,\" he replied, \"how does this thing work?\" \"Simple,\" she said, taking the fat report from his hand and feeding it into the shredder. \"Thanks, but where do the copies come out?\"', 'A guy walked past a mental hospital and heard a moaning voice: \"13...13...13...13...\" The man looked over to the hospital and saw a hole in the wall. He looked through the hole and got poked in the eye. The moaning voice then groaned: \"14...14...14...14...\"', 'A man went to apply for a job. After filling out all of his applications, he waited anxiously for the outcome. The employer read all his applications and said, \"We have an opening for people like you.\" \"Oh, great,\" he said. \"What is it?\" \"It\\'s called the door!\"', 'Deep within a forest, a little turtle began to climb a tree. After hours of effort, he reached the top, jumped into the air waving his front legs and crashed to the ground. After recovering, he slowly climbed the tree again, jumped, and fell to the ground. The turtle tried again and again, while a couple of birds sitting on a branch watched his sad efforts. Finally, the female bird turned to her mate. \"Dear,\" she chirped, \"I think it\\'s time to tell him he\\'s adopted.\"', 'A man is driving in the country one evening when his car stalls and won\\'t start. He goes up to a nearby farm house for help, and because it is suppertime he is asked to stay for supper. When he sits down at the table he notices that a pig is sitting at the table with them for supper and that the pig has a wooden leg. As they are eating and chatting, he eventually asks the farmer why the pig is there and why it has a wooden leg. \"Oh,\" says the farmer, \"that is a very special pig. Last month my wife and daughter were in the barn when it caught fire. The pig saw this, ran to the barn, tipped over a pail of water, crawled over the wet floor to reach them and pulled them out of the barn safely. A special pig like that, you just don\\'t eat it all at once!\"', 'It was the day of the big sale. Rumors of the sale (and some advertising in the local paper) were the main reason for the long line that formed by 8:30, the store\\'s opening time, in front of the store. A small man pushed his way to the front of the line, only to be pushed back, amid loud and colorful curses. On the man\\'s second attempt, he was punched square in the jaw, and knocked around a bit, and then thrown to the end of the line again. As he got up the second time, he said to the person at the end of the line... \"That does it! If they hit me one more time, I won\\'t open the store!\"']\n",
      " \n",
      " JokesfromSimilarUsers ecluding movies of user \n",
      "         jokeId                                           jokeText  \\\n",
      "420246       32  A man arrives at the gates of heaven. St. Pete...   \n",
      "420250       47  There was an engineer who had an exceptional g...   \n",
      "420247       54  The Pope dies and, naturally, goes to heaven. ...   \n",
      "420253       72  On the first day of college, the Dean addresse...   \n",
      "420223       89  A radio conversation of a US naval \\nship with...   \n",
      "420248      106  An engineer dies and reports to the pearly gat...   \n",
      "420245      114  Sherlock Holmes and Dr. Watson go on a camping...   \n",
      "420271      139  In a Veteran's Day speech, President Bush vowe...   \n",
      "420264      148  Recently a teacher, a garbage collector, and a...   \n",
      "420249      150  In an interview with David Letterman, Carter p...   \n",
      "423035       36  A guy walks into a bar, orders a beer and says...   \n",
      "423028       53  One Sunday morning William burst into the livi...   \n",
      "423029       66  A lawyer opened the door of his BMW, when sudd...   \n",
      "423030       69  This guys wife asks, \"Honey if I died would yo...   \n",
      "423070      102  A man escaped jail by digging a hole from his ...   \n",
      "423032      117  A man joins a big corporate empire as a traine...   \n",
      "423031      129  A group of girlfriends is on vacation when the...   \n",
      "423034      143  A preist, a 12-year-old kid, and the smartest ...   \n",
      "438829       24  What do you get when you run over a parakeet w...   \n",
      "438786       30  Q: What's the difference between a Lawyer and ...   \n",
      "438774       35  An explorer in the deepest Amazon suddenly fin...   \n",
      "438777       44  A horse walks into a bar. Bartender says:\\n\"So...   \n",
      "438776       58  How many teddybears does it take to change a l...   \n",
      "438785       59  The Chukcha (Russian Eskimo) phones up the Rus...   \n",
      "438837       65  Two Rednecks were seated at the end of a bar w...   \n",
      "438768       76  There once was a man and a woman that both  go...   \n",
      "438782       77  If pro- is the opposite of con- then congress ...   \n",
      "438789       79  Q: Ever wonder why the IRS calls it Form 1040?...   \n",
      "438783       84  Q: What is the difference between Mechanical E...   \n",
      "438779       88  A Czechoslovakian man felt his eyesight was gr...   \n",
      "438780       90  Q: How many programmers does it take to change...   \n",
      "438772      134  An artist asked the gallery owner if there had...   \n",
      "438775      145  A blonde, brunette, and a red head are all lin...   \n",
      "438788      146  America: 8:00 - Welcome to work! 12:00 - Lunch...   \n",
      "696182      105  A couple of hunters are out in the woods in th...   \n",
      "696188      128  An American, a Scot and a Canadian were in a t...   \n",
      "756754      127  A little boy goes to his dad and asks, \"What i...   \n",
      "1110237      64  What is the rallying cry of the International ...   \n",
      "1110238      85  Q: How many Presidents does it take to screw i...   \n",
      "1110239     101  Did you hear about the Buddhist who refused No...   \n",
      "1118991      21  What's the difference between a used tire and ...   \n",
      "1119033      26  A guy walks into a bar and sits down next to a...   \n",
      "1119031      81  An Asian man goes into a New York CityBank to ...   \n",
      "1118992      83  What a woman says:\\n\\n\"This place is a mess!  ...   \n",
      "1119030      87  A man, recently completing a routine physical ...   \n",
      "1119032      98  Age and Womanhood\\n\\n1. Between the ages of 13...   \n",
      "1119034     125  An American tourist goes into a restaurant in ...   \n",
      "1118989     132  Mickey Mouse is having a nasty divorce with Mi...   \n",
      "1119029     138  WASHINGTON (Reuters) - A tragic fire on Monday...   \n",
      "1118993     142  One day, three men went to a shrine to ask the...   \n",
      "1148250      57  Why are there so many Jones's in the phone boo...   \n",
      "1148170     115  A lady bought a new Lexus. It cost a bundle. T...   \n",
      "1148173     120  Judy was having trouble with her computer, so ...   \n",
      "1148275     123  When most people claim to be \"killing time\", i...   \n",
      "1148251     124  Person 1: Hey, wanna hear a great knock-knock ...   \n",
      "1148274     140  Chuck Norris' calendar goes straight from Marc...   \n",
      "1148276     141  Jack Bauer can get McDonald's breakfast after ...   \n",
      "1148171     149  A little girl asked her father, \"Daddy? Do all...   \n",
      "\n",
      "         avg_rating_by_similar_users  \n",
      "420246                      4.958333  \n",
      "420250                      5.142333  \n",
      "420247                      3.862600  \n",
      "420253                      4.839875  \n",
      "420223                      5.482556  \n",
      "420248                      5.111222  \n",
      "420245                      3.944444  \n",
      "420271                      2.148250  \n",
      "420264                      4.046750  \n",
      "420249                      4.854111  \n",
      "423035                      0.862400  \n",
      "423028                      5.472625  \n",
      "423029                      3.683500  \n",
      "423030                      4.281222  \n",
      "423070                      5.521000  \n",
      "423032                      6.968714  \n",
      "423031                      3.031286  \n",
      "423034                      2.921875  \n",
      "438829                      3.211000  \n",
      "438786                      2.880167  \n",
      "438774                      6.004571  \n",
      "438777                      1.656000  \n",
      "438776                     -0.289250  \n",
      "438785                      4.973833  \n",
      "438837                      6.425000  \n",
      "438768                      4.513286  \n",
      "438782                      1.932333  \n",
      "438789                      4.744833  \n",
      "438783                      4.036500  \n",
      "438779                      6.116286  \n",
      "438780                      4.349000  \n",
      "438772                      4.633714  \n",
      "438775                      3.035714  \n",
      "438788                      1.400833  \n",
      "696182                      4.468667  \n",
      "696188                      4.962400  \n",
      "756754                      3.937500  \n",
      "1110237                     3.023500  \n",
      "1110238                     1.679500  \n",
      "1110239                     3.414250  \n",
      "1118991                     3.047000  \n",
      "1119033                     6.609500  \n",
      "1119031                     5.843750  \n",
      "1118992                     1.570250  \n",
      "1119030                     4.960750  \n",
      "1119032                     1.328000  \n",
      "1119034                     6.078250  \n",
      "1118989                     2.828250  \n",
      "1119029                     1.828000  \n",
      "1118993                     2.593500  \n",
      "1148250                     2.291667  \n",
      "1148170                     5.260667  \n",
      "1148173                    -0.791667  \n",
      "1148275                     3.354000  \n",
      "1148251                     3.854333  \n",
      "1148274                     3.708333  \n",
      "1148276                     1.604000  \n",
      "1148171                     4.385667  \n",
      " \n",
      " recommended_joke \n",
      "A man joins a big corporate empire as a trainee. On his very first day of work, he dials the pantry and shouts into the phone: \"Get me a coffee, quickly!\" The voice from the other side responds, \"You fool, you've dialed the wrong extension! Do you know who you're talking to, dumbo?\" \"No,\" replied the trainee. \"It's the CEO of the company, you fool!\" The trainee shouts back, \"And do YOU know who YOU are talking to, you fool?!\" \"No.\" replied the CEO indignantly. \"Good!\" replied the trainee, and puts down the phone.  with Rating by Similar Users:  6.968714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3343270999.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n"
     ]
    }
   ],
   "source": [
    "# Getting the  list of unique user IDs in training data\n",
    "user_ids = train_data['userId'].unique()\n",
    "user_ids.sort()\n",
    "user_ids=user_ids.tolist()\n",
    "\n",
    "#Getting the Joke Recommandation based on KNN to a Random User from the list of User IDs \n",
    "#get_KNN_recommandations_for_user(35560)\n",
    "input_user_index = user_ids.index(35560)\n",
    "print('\\n Input users index is ', input_user_index)\n",
    "neighbor_count=10\n",
    "# Finding the k nearest neighbors of the input user \n",
    "distances, indices = knn.kneighbors(jokes_user_train.iloc[input_user_index,:].values.reshape(1,-1), n_neighbors=neighbor_count+1)\n",
    "print(' \\n distances are ')\n",
    "print(distances.flatten())\n",
    "\n",
    "print(' \\n indices are')\n",
    "print(indices.flatten())\n",
    "\n",
    "similaruser_ids=[]\n",
    "    # Printing the most similar users to the input user based on the calculated distances\n",
    "for i in range(0,len(distances.flatten())):\n",
    "    if i==0:            #Most similar user to input user is itself with zero distance. So not printing the input user \n",
    "        print(' \\n Similar Users for User',jokes_user_train.index[indices.flatten()[i]],' \\n')\n",
    "    else:\n",
    "        print( 'Most Similar User',i,': User_Id ' +  str(jokes_user_train.index[indices.flatten()[i]]),' with distance of ',str(distances.flatten()[i]) )\n",
    "        similaruser_ids.append(jokes_user_train.index[indices.flatten()[i]])\n",
    "\n",
    "print(' \\n similaruserid list ')\n",
    "\n",
    "print(similaruser_ids)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #Filtering the data to only include the rows of similar users\n",
    "similar_user_data=None\n",
    "similar_user_data = train_data[train_data[\"userId\"].isin(similaruser_ids)]\n",
    "#Computing the Average Rating of Jokes among the similar users \n",
    "similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
    "print(' \\n similar_user_data ')\n",
    "print(similar_user_data)\n",
    "\n",
    "# Getting jokes rated by similar users and   excluding jokes that already rated by input_user\n",
    "JokesfromSimilarUsers=None\n",
    "JokesfromSimilarUsers= similar_user_data[[\"jokeId\",'jokeText','avg_rating_by_similar_users']].drop_duplicates(subset=['jokeId'])\n",
    "print(' \\n JokesfromSimilarUsers ')\n",
    "print(JokesfromSimilarUsers)\n",
    "JokesRatedBySelectedUser=train_data[(train_data['userId'] == 35560)]['jokeText'].tolist()\n",
    "print(' \\n JokesRatedBySelectedUser ')\n",
    "print(JokesRatedBySelectedUser)\n",
    "JokesfromSimilarUsers = JokesfromSimilarUsers[~JokesfromSimilarUsers['jokeText'].isin(JokesRatedBySelectedUser)]\n",
    "print(' \\n JokesfromSimilarUsers ecluding movies of user ')\n",
    "print(JokesfromSimilarUsers)\n",
    "\n",
    "# Getting the recommended joke with the highest average rating among similar user\n",
    "recommended_joke=None\n",
    "recommended_joke = get_Jokes_with_Highhest_Rating_from_Similar_Users(JokesfromSimilarUsers)\n",
    "RatingbySimilarUsers=JokesfromSimilarUsers[(JokesfromSimilarUsers['jokeText'] == recommended_joke)]['avg_rating_by_similar_users'].iloc[0]\n",
    "\n",
    "print(' \\n recommended_joke ')\n",
    "print(recommended_joke, ' with Rating by Similar Users: ' , RatingbySimilarUsers)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.2 Evaluation of KNN Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the KNN model and to measure its effectiveness in generating accurate recommendations, a comparison will be made between the predicted ratings and the actual ratings for a predefined set of users and jokes.\n",
    "\n",
    " The objective is to identify the users for whom the model recommends a joke that they have already rated. Predicted ratings will be the average ranking of jokes among the similar users. This  allows  to calculate the Root Mean Squared Error (RMSE) between the user's actual rating and the predicted rating for the recommended joke.\n",
    "\n",
    "By calculating the RMSE, it is possible to determine how closely the predicted ratings match the actual ratings. A lower RMSE indicates better accuracy and suggests that the model is doing a good job of recommending jokes that the user would rate highly. This information can be used to fine-tune the model to further improve its accuracy and to provide better recommendations to users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data into a sparse matrix format\n",
    "csrtest = csr_matrix(jokes_user_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>jokeId</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49941</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.125</td>\n",
       "      <td>-7.844</td>\n",
       "      <td>-8.062</td>\n",
       "      <td>-8.281</td>\n",
       "      <td>7.406</td>\n",
       "      <td>-4.344</td>\n",
       "      <td>-5.594</td>\n",
       "      <td>5.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.250</td>\n",
       "      <td>-7.031</td>\n",
       "      <td>-5.812</td>\n",
       "      <td>-2.906</td>\n",
       "      <td>-6.719</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.250</td>\n",
       "      <td>-1.938</td>\n",
       "      <td>-2.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.812</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>6.875</td>\n",
       "      <td>9.812</td>\n",
       "      <td>7.938</td>\n",
       "      <td>-8.281</td>\n",
       "      <td>4.719</td>\n",
       "      <td>-2.812</td>\n",
       "      <td>4.719</td>\n",
       "      <td>-6.688</td>\n",
       "      <td>-2.344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.156</td>\n",
       "      <td>6.750</td>\n",
       "      <td>4.531</td>\n",
       "      <td>-2.969</td>\n",
       "      <td>-2.719</td>\n",
       "      <td>2.719</td>\n",
       "      <td>-2.312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.125</td>\n",
       "      <td>-3.906</td>\n",
       "      <td>-7.875</td>\n",
       "      <td>6.781</td>\n",
       "      <td>-3.062</td>\n",
       "      <td>4.938</td>\n",
       "      <td>2.469</td>\n",
       "      <td>-3.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.812</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-3.625</td>\n",
       "      <td>-4.188</td>\n",
       "      <td>7.594</td>\n",
       "      <td>-8.062</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>2.062</td>\n",
       "      <td>5.406</td>\n",
       "      <td>-6.094</td>\n",
       "      <td>8.344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.031</td>\n",
       "      <td>3.156</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>-6.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.312</td>\n",
       "      <td>2.938</td>\n",
       "      <td>7.000</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>-4.031</td>\n",
       "      <td>3.250</td>\n",
       "      <td>-2.469</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1.344</td>\n",
       "      <td>-5.531</td>\n",
       "      <td>5.656</td>\n",
       "      <td>6.094</td>\n",
       "      <td>-1.781</td>\n",
       "      <td>-2.875</td>\n",
       "      <td>7.719</td>\n",
       "      <td>-3.375</td>\n",
       "      <td>3.406</td>\n",
       "      <td>-6.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.438</td>\n",
       "      <td>0.062</td>\n",
       "      <td>5.469</td>\n",
       "      <td>3.781</td>\n",
       "      <td>6.625</td>\n",
       "      <td>6.344</td>\n",
       "      <td>-3.094</td>\n",
       "      <td>2.375</td>\n",
       "      <td>7.938</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>4.938</td>\n",
       "      <td>-2.438</td>\n",
       "      <td>1.656</td>\n",
       "      <td>5.312</td>\n",
       "      <td>6.406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.781</td>\n",
       "      <td>4.875</td>\n",
       "      <td>6.188</td>\n",
       "      <td>4.500</td>\n",
       "      <td>8.406</td>\n",
       "      <td>-2.062</td>\n",
       "      <td>-6.844</td>\n",
       "      <td>-5.969</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>5.250</td>\n",
       "      <td>6.688</td>\n",
       "      <td>0.281</td>\n",
       "      <td>8.438</td>\n",
       "      <td>8.969</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>8.500</td>\n",
       "      <td>2.469</td>\n",
       "      <td>-1.656</td>\n",
       "      <td>-2.062</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-4.812</td>\n",
       "      <td>-6.938</td>\n",
       "      <td>-6.469</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>5.938</td>\n",
       "      <td>8.125</td>\n",
       "      <td>5.125</td>\n",
       "      <td>-2.969</td>\n",
       "      <td>-3.688</td>\n",
       "      <td>0.969</td>\n",
       "      <td>-2.562</td>\n",
       "      <td>4.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49946</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.875</td>\n",
       "      <td>-9.781</td>\n",
       "      <td>-9.812</td>\n",
       "      <td>-9.812</td>\n",
       "      <td>-9.875</td>\n",
       "      <td>-9.875</td>\n",
       "      <td>-9.781</td>\n",
       "      <td>-9.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.562</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.906</td>\n",
       "      <td>9.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6.625</td>\n",
       "      <td>9.156</td>\n",
       "      <td>-9.000</td>\n",
       "      <td>2.844</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.562</td>\n",
       "      <td>-9.656</td>\n",
       "      <td>8.656</td>\n",
       "      <td>4.531</td>\n",
       "      <td>8.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.312</td>\n",
       "      <td>-9.750</td>\n",
       "      <td>-4.750</td>\n",
       "      <td>9.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.844</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.719</td>\n",
       "      <td>-5.625</td>\n",
       "      <td>9.594</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>6.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.562</td>\n",
       "      <td>1.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.719</td>\n",
       "      <td>9.375</td>\n",
       "      <td>4.312</td>\n",
       "      <td>3.438</td>\n",
       "      <td>5.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.625</td>\n",
       "      <td>-9.656</td>\n",
       "      <td>9.500</td>\n",
       "      <td>9.938</td>\n",
       "      <td>9.875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.969</td>\n",
       "      <td>7.906</td>\n",
       "      <td>9.750</td>\n",
       "      <td>9.938</td>\n",
       "      <td>9.969</td>\n",
       "      <td>9.781</td>\n",
       "      <td>9.875</td>\n",
       "      <td>9.844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.750</td>\n",
       "      <td>9.719</td>\n",
       "      <td>9.844</td>\n",
       "      <td>9.500</td>\n",
       "      <td>9.562</td>\n",
       "      <td>9.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.719</td>\n",
       "      <td>9.688</td>\n",
       "      <td>9.312</td>\n",
       "      <td>9.906</td>\n",
       "      <td>8.125</td>\n",
       "      <td>-5.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.531</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-9.531</td>\n",
       "      <td>4.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.812</td>\n",
       "      <td>8.156</td>\n",
       "      <td>-9.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.156</td>\n",
       "      <td>9.844</td>\n",
       "      <td>-9.688</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.719</td>\n",
       "      <td>8.969</td>\n",
       "      <td>-9.688</td>\n",
       "      <td>9.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49961</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.125</td>\n",
       "      <td>5.781</td>\n",
       "      <td>4.125</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>2.219</td>\n",
       "      <td>-2.281</td>\n",
       "      <td>2.375</td>\n",
       "      <td>3.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.844</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.844</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.812</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.312</td>\n",
       "      <td>-6.812</td>\n",
       "      <td>-9.844</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.406</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.531</td>\n",
       "      <td>6.594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.125</td>\n",
       "      <td>5.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.719</td>\n",
       "      <td>1.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.594</td>\n",
       "      <td>4.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.531</td>\n",
       "      <td>3.562</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>5.969</td>\n",
       "      <td>-4.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.219</td>\n",
       "      <td>4.938</td>\n",
       "      <td>5.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.375</td>\n",
       "      <td>2.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>7.031</td>\n",
       "      <td>3.750</td>\n",
       "      <td>4.938</td>\n",
       "      <td>-5.406</td>\n",
       "      <td>5.844</td>\n",
       "      <td>-2.906</td>\n",
       "      <td>5.219</td>\n",
       "      <td>5.625</td>\n",
       "      <td>-2.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.625</td>\n",
       "      <td>-5.094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.438</td>\n",
       "      <td>-8.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9.375</td>\n",
       "      <td>-9.625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.906</td>\n",
       "      <td>-5.438</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-7.781</td>\n",
       "      <td>2.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6.188</td>\n",
       "      <td>0.031</td>\n",
       "      <td>5.656</td>\n",
       "      <td>-9.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-7.844</td>\n",
       "      <td>2.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.688</td>\n",
       "      <td>2.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.281</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-3.562</td>\n",
       "      <td>-0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49963</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.531</td>\n",
       "      <td>0.188</td>\n",
       "      <td>4.688</td>\n",
       "      <td>1.719</td>\n",
       "      <td>5.688</td>\n",
       "      <td>4.438</td>\n",
       "      <td>4.812</td>\n",
       "      <td>3.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312</td>\n",
       "      <td>9.781</td>\n",
       "      <td>4.125</td>\n",
       "      <td>0.219</td>\n",
       "      <td>6.500</td>\n",
       "      <td>7.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.781</td>\n",
       "      <td>1.719</td>\n",
       "      <td>1.312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>1.312</td>\n",
       "      <td>2.250</td>\n",
       "      <td>8.531</td>\n",
       "      <td>6.094</td>\n",
       "      <td>9.719</td>\n",
       "      <td>9.812</td>\n",
       "      <td>7.875</td>\n",
       "      <td>2.094</td>\n",
       "      <td>6.719</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>1.031</td>\n",
       "      <td>2.281</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>9.750</td>\n",
       "      <td>5.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.812</td>\n",
       "      <td>2.156</td>\n",
       "      <td>9.812</td>\n",
       "      <td>-6.719</td>\n",
       "      <td>-1.469</td>\n",
       "      <td>5.469</td>\n",
       "      <td>0.031</td>\n",
       "      <td>9.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.562</td>\n",
       "      <td>3.688</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>-9.500</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>9.781</td>\n",
       "      <td>5.875</td>\n",
       "      <td>9.156</td>\n",
       "      <td>7.312</td>\n",
       "      <td>9.812</td>\n",
       "      <td>7.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>6.344</td>\n",
       "      <td>9.750</td>\n",
       "      <td>8.562</td>\n",
       "      <td>6.656</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.750</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.656</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.375</td>\n",
       "      <td>6.969</td>\n",
       "      <td>9.688</td>\n",
       "      <td>5.406</td>\n",
       "      <td>1.406</td>\n",
       "      <td>9.781</td>\n",
       "      <td>5.469</td>\n",
       "      <td>4.781</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.375</td>\n",
       "      <td>8.562</td>\n",
       "      <td>0.156</td>\n",
       "      <td>9.812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.500</td>\n",
       "      <td>7.594</td>\n",
       "      <td>9.781</td>\n",
       "      <td>0.906</td>\n",
       "      <td>5.594</td>\n",
       "      <td>3.844</td>\n",
       "      <td>0.438</td>\n",
       "      <td>7.344</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.094</td>\n",
       "      <td>1.688</td>\n",
       "      <td>7.156</td>\n",
       "      <td>9.781</td>\n",
       "      <td>9.625</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.250</td>\n",
       "      <td>0.844</td>\n",
       "      <td>2.688</td>\n",
       "      <td>9.719</td>\n",
       "      <td>9.719</td>\n",
       "      <td>7.406</td>\n",
       "      <td>4.531</td>\n",
       "      <td>2.750</td>\n",
       "      <td>5.406</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.375</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>7.906</td>\n",
       "      <td>8.562</td>\n",
       "      <td>9.750</td>\n",
       "      <td>8.562</td>\n",
       "      <td>6.188</td>\n",
       "      <td>8.531</td>\n",
       "      <td>1.094</td>\n",
       "      <td>9.781</td>\n",
       "      <td>4.938</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>-4.844</td>\n",
       "      <td>4.312</td>\n",
       "      <td>1.438</td>\n",
       "      <td>7.594</td>\n",
       "      <td>9.781</td>\n",
       "      <td>-4.688</td>\n",
       "      <td>-2.469</td>\n",
       "      <td>2.031</td>\n",
       "      <td>6.969</td>\n",
       "      <td>1.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.406</td>\n",
       "      <td>-4.125</td>\n",
       "      <td>-5.469</td>\n",
       "      <td>1.594</td>\n",
       "      <td>-1.906</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>1.469</td>\n",
       "      <td>-2.656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.219</td>\n",
       "      <td>-2.844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.531</td>\n",
       "      <td>2.062</td>\n",
       "      <td>-4.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>4.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.844</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>-2.625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-9.469</td>\n",
       "      <td>4.438</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.656</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.094</td>\n",
       "      <td>-3.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.469</td>\n",
       "      <td>4.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>3.500</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.312</td>\n",
       "      <td>-4.781</td>\n",
       "      <td>-2.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.656</td>\n",
       "      <td>-3.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.562</td>\n",
       "      <td>2.781</td>\n",
       "      <td>3.219</td>\n",
       "      <td>4.719</td>\n",
       "      <td>-3.531</td>\n",
       "      <td>-2.469</td>\n",
       "      <td>2.156</td>\n",
       "      <td>-3.656</td>\n",
       "      <td>1.562</td>\n",
       "      <td>-1.281</td>\n",
       "      <td>2.562</td>\n",
       "      <td>-3.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.469</td>\n",
       "      <td>3.938</td>\n",
       "      <td>-3.531</td>\n",
       "      <td>2.656</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>1.688</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>0.625</td>\n",
       "      <td>2.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.969</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>-2.250</td>\n",
       "      <td>-2.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-5.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.250</td>\n",
       "      <td>0.938</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>-4.656</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.250</td>\n",
       "      <td>-2.844</td>\n",
       "      <td>2.312</td>\n",
       "      <td>-2.562</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-4.656</td>\n",
       "      <td>1.844</td>\n",
       "      <td>-4.781</td>\n",
       "      <td>-3.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.719</td>\n",
       "      <td>-3.688</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>-1.938</td>\n",
       "      <td>-3.531</td>\n",
       "      <td>1.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63944</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>3.094</td>\n",
       "      <td>9.656</td>\n",
       "      <td>8.000</td>\n",
       "      <td>-5.375</td>\n",
       "      <td>4.438</td>\n",
       "      <td>6.781</td>\n",
       "      <td>3.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.719</td>\n",
       "      <td>6.156</td>\n",
       "      <td>6.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.375</td>\n",
       "      <td>5.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.688</td>\n",
       "      <td>8.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>4.688</td>\n",
       "      <td>4.688</td>\n",
       "      <td>2.812</td>\n",
       "      <td>9.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.656</td>\n",
       "      <td>7.188</td>\n",
       "      <td>8.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.125</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>6.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.656</td>\n",
       "      <td>7.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.312</td>\n",
       "      <td>5.906</td>\n",
       "      <td>4.688</td>\n",
       "      <td>5.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.031</td>\n",
       "      <td>4.219</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.562</td>\n",
       "      <td>4.594</td>\n",
       "      <td>3.750</td>\n",
       "      <td>4.344</td>\n",
       "      <td>5.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.938</td>\n",
       "      <td>3.031</td>\n",
       "      <td>5.844</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.969</td>\n",
       "      <td>7.188</td>\n",
       "      <td>4.688</td>\n",
       "      <td>6.500</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.625</td>\n",
       "      <td>4.344</td>\n",
       "      <td>3.469</td>\n",
       "      <td>2.688</td>\n",
       "      <td>1.469</td>\n",
       "      <td>0.031</td>\n",
       "      <td>6.062</td>\n",
       "      <td>5.406</td>\n",
       "      <td>1.438</td>\n",
       "      <td>0.062</td>\n",
       "      <td>6.125</td>\n",
       "      <td>4.156</td>\n",
       "      <td>5.750</td>\n",
       "      <td>4.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.375</td>\n",
       "      <td>4.562</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>7.156</td>\n",
       "      <td>6.750</td>\n",
       "      <td>5.375</td>\n",
       "      <td>4.969</td>\n",
       "      <td>6.312</td>\n",
       "      <td>4.812</td>\n",
       "      <td>2.656</td>\n",
       "      <td>1.500</td>\n",
       "      <td>6.062</td>\n",
       "      <td>4.375</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.438</td>\n",
       "      <td>4.969</td>\n",
       "      <td>7.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.719</td>\n",
       "      <td>3.969</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.375</td>\n",
       "      <td>5.375</td>\n",
       "      <td>6.531</td>\n",
       "      <td>4.594</td>\n",
       "      <td>2.375</td>\n",
       "      <td>6.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.906</td>\n",
       "      <td>5.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.562</td>\n",
       "      <td>4.625</td>\n",
       "      <td>4.562</td>\n",
       "      <td>5.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.125</td>\n",
       "      <td>3.969</td>\n",
       "      <td>9.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.656</td>\n",
       "      <td>5.906</td>\n",
       "      <td>6.094</td>\n",
       "      <td>6.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63947</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.906</td>\n",
       "      <td>-5.625</td>\n",
       "      <td>-6.438</td>\n",
       "      <td>-5.688</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>3.156</td>\n",
       "      <td>3.156</td>\n",
       "      <td>7.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>8.906</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.844</td>\n",
       "      <td>9.031</td>\n",
       "      <td>8.875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.469</td>\n",
       "      <td>9.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.719</td>\n",
       "      <td>8.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.219</td>\n",
       "      <td>9.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.875</td>\n",
       "      <td>9.656</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.688</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.219</td>\n",
       "      <td>9.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.844</td>\n",
       "      <td>8.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.625</td>\n",
       "      <td>8.938</td>\n",
       "      <td>8.719</td>\n",
       "      <td>9.219</td>\n",
       "      <td>9.156</td>\n",
       "      <td>9.250</td>\n",
       "      <td>9.062</td>\n",
       "      <td>9.312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.844</td>\n",
       "      <td>9.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.938</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.281</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.438</td>\n",
       "      <td>8.531</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>9.875</td>\n",
       "      <td>9.875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.625</td>\n",
       "      <td>9.219</td>\n",
       "      <td>-7.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.375</td>\n",
       "      <td>9.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.938</td>\n",
       "      <td>9.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.562</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.438</td>\n",
       "      <td>9.156</td>\n",
       "      <td>8.781</td>\n",
       "      <td>8.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63949</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.094</td>\n",
       "      <td>-6.031</td>\n",
       "      <td>1.344</td>\n",
       "      <td>5.344</td>\n",
       "      <td>3.594</td>\n",
       "      <td>-6.406</td>\n",
       "      <td>3.062</td>\n",
       "      <td>3.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.562</td>\n",
       "      <td>-5.125</td>\n",
       "      <td>-5.594</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-5.125</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.938</td>\n",
       "      <td>-6.375</td>\n",
       "      <td>5.281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.750</td>\n",
       "      <td>-5.625</td>\n",
       "      <td>-5.125</td>\n",
       "      <td>-4.688</td>\n",
       "      <td>5.281</td>\n",
       "      <td>5.281</td>\n",
       "      <td>1.500</td>\n",
       "      <td>6.500</td>\n",
       "      <td>4.000</td>\n",
       "      <td>-5.969</td>\n",
       "      <td>-5.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.938</td>\n",
       "      <td>8.312</td>\n",
       "      <td>-5.594</td>\n",
       "      <td>5.375</td>\n",
       "      <td>6.219</td>\n",
       "      <td>5.906</td>\n",
       "      <td>5.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-5.562</td>\n",
       "      <td>4.281</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.156</td>\n",
       "      <td>6.219</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.906</td>\n",
       "      <td>5.500</td>\n",
       "      <td>-5.938</td>\n",
       "      <td>6.312</td>\n",
       "      <td>-3.656</td>\n",
       "      <td>4.375</td>\n",
       "      <td>7.031</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>5.406</td>\n",
       "      <td>6.219</td>\n",
       "      <td>3.719</td>\n",
       "      <td>-5.875</td>\n",
       "      <td>6.906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.500</td>\n",
       "      <td>5.281</td>\n",
       "      <td>-3.250</td>\n",
       "      <td>-4.562</td>\n",
       "      <td>-4.438</td>\n",
       "      <td>4.906</td>\n",
       "      <td>-5.750</td>\n",
       "      <td>0.719</td>\n",
       "      <td>5.781</td>\n",
       "      <td>3.750</td>\n",
       "      <td>5.500</td>\n",
       "      <td>5.500</td>\n",
       "      <td>6.062</td>\n",
       "      <td>2.062</td>\n",
       "      <td>6.094</td>\n",
       "      <td>5.094</td>\n",
       "      <td>7.812</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>4.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.906</td>\n",
       "      <td>6.500</td>\n",
       "      <td>5.281</td>\n",
       "      <td>6.406</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>5.594</td>\n",
       "      <td>6.219</td>\n",
       "      <td>3.312</td>\n",
       "      <td>7.625</td>\n",
       "      <td>8.281</td>\n",
       "      <td>6.156</td>\n",
       "      <td>7.625</td>\n",
       "      <td>5.875</td>\n",
       "      <td>6.344</td>\n",
       "      <td>-4.969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.188</td>\n",
       "      <td>6.094</td>\n",
       "      <td>-2.812</td>\n",
       "      <td>6.969</td>\n",
       "      <td>5.219</td>\n",
       "      <td>6.375</td>\n",
       "      <td>-5.969</td>\n",
       "      <td>-5.969</td>\n",
       "      <td>6.219</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.938</td>\n",
       "      <td>4.375</td>\n",
       "      <td>4.656</td>\n",
       "      <td>6.594</td>\n",
       "      <td>5.281</td>\n",
       "      <td>-2.812</td>\n",
       "      <td>6.844</td>\n",
       "      <td>4.188</td>\n",
       "      <td>0.812</td>\n",
       "      <td>7.156</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>-5.781</td>\n",
       "      <td>3.750</td>\n",
       "      <td>-6.469</td>\n",
       "      <td>-5.594</td>\n",
       "      <td>1.375</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1.188</td>\n",
       "      <td>3.469</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-5.594</td>\n",
       "      <td>4.688</td>\n",
       "      <td>2.062</td>\n",
       "      <td>5.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.375</td>\n",
       "      <td>2.406</td>\n",
       "      <td>3.562</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.469</td>\n",
       "      <td>3.219</td>\n",
       "      <td>5.031</td>\n",
       "      <td>2.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.938</td>\n",
       "      <td>4.031</td>\n",
       "      <td>3.969</td>\n",
       "      <td>-4.969</td>\n",
       "      <td>2.125</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.781</td>\n",
       "      <td>5.062</td>\n",
       "      <td>2.156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.438</td>\n",
       "      <td>3.594</td>\n",
       "      <td>1.156</td>\n",
       "      <td>3.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.938</td>\n",
       "      <td>2.188</td>\n",
       "      <td>3.438</td>\n",
       "      <td>-4.188</td>\n",
       "      <td>1.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.531</td>\n",
       "      <td>4.156</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.625</td>\n",
       "      <td>4.188</td>\n",
       "      <td>3.875</td>\n",
       "      <td>3.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.406</td>\n",
       "      <td>1.969</td>\n",
       "      <td>0.719</td>\n",
       "      <td>2.438</td>\n",
       "      <td>1.344</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>3.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.562</td>\n",
       "      <td>1.938</td>\n",
       "      <td>-2.031</td>\n",
       "      <td>3.688</td>\n",
       "      <td>5.469</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.656</td>\n",
       "      <td>2.719</td>\n",
       "      <td>3.344</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.938</td>\n",
       "      <td>3.688</td>\n",
       "      <td>1.156</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.594</td>\n",
       "      <td>-4.312</td>\n",
       "      <td>3.250</td>\n",
       "      <td>0.188</td>\n",
       "      <td>2.938</td>\n",
       "      <td>2.719</td>\n",
       "      <td>6.188</td>\n",
       "      <td>-3.062</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.688</td>\n",
       "      <td>1.594</td>\n",
       "      <td>3.656</td>\n",
       "      <td>4.000</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.531</td>\n",
       "      <td>3.031</td>\n",
       "      <td>-3.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.125</td>\n",
       "      <td>2.594</td>\n",
       "      <td>4.844</td>\n",
       "      <td>2.188</td>\n",
       "      <td>6.062</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2.062</td>\n",
       "      <td>1.688</td>\n",
       "      <td>1.969</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>0.938</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.031</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.125</td>\n",
       "      <td>4.062</td>\n",
       "      <td>2.031</td>\n",
       "      <td>-3.906</td>\n",
       "      <td>-4.594</td>\n",
       "      <td>1.031</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-7.031</td>\n",
       "      <td>1.781</td>\n",
       "      <td>3.375</td>\n",
       "      <td>5.688</td>\n",
       "      <td>-2.719</td>\n",
       "      <td>6.469</td>\n",
       "      <td>-1.656</td>\n",
       "      <td>5.719</td>\n",
       "      <td>1.969</td>\n",
       "      <td>1.938</td>\n",
       "      <td>3.062</td>\n",
       "      <td>4.656</td>\n",
       "      <td>6.625</td>\n",
       "      <td>-1.625</td>\n",
       "      <td>8.188</td>\n",
       "      <td>2.438</td>\n",
       "      <td>-3.438</td>\n",
       "      <td>-5.031</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.375</td>\n",
       "      <td>0.875</td>\n",
       "      <td>3.625</td>\n",
       "      <td>2.281</td>\n",
       "      <td>-4.250</td>\n",
       "      <td>1.812</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.906</td>\n",
       "      <td>-7.594</td>\n",
       "      <td>-7.594</td>\n",
       "      <td>-6.375</td>\n",
       "      <td>-6.375</td>\n",
       "      <td>-6.375</td>\n",
       "      <td>-6.375</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.438</td>\n",
       "      <td>9.156</td>\n",
       "      <td>8.844</td>\n",
       "      <td>-9.062</td>\n",
       "      <td>8.938</td>\n",
       "      <td>8.906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.969</td>\n",
       "      <td>9.312</td>\n",
       "      <td>8.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.531</td>\n",
       "      <td>-7.469</td>\n",
       "      <td>7.500</td>\n",
       "      <td>6.938</td>\n",
       "      <td>8.969</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>-5.156</td>\n",
       "      <td>9.156</td>\n",
       "      <td>8.375</td>\n",
       "      <td>9.156</td>\n",
       "      <td>9.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.438</td>\n",
       "      <td>-6.594</td>\n",
       "      <td>8.062</td>\n",
       "      <td>7.156</td>\n",
       "      <td>8.406</td>\n",
       "      <td>8.969</td>\n",
       "      <td>8.562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.719</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-7.750</td>\n",
       "      <td>8.500</td>\n",
       "      <td>-8.531</td>\n",
       "      <td>-8.656</td>\n",
       "      <td>8.844</td>\n",
       "      <td>-7.281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.188</td>\n",
       "      <td>8.250</td>\n",
       "      <td>-8.188</td>\n",
       "      <td>-7.781</td>\n",
       "      <td>8.250</td>\n",
       "      <td>-6.812</td>\n",
       "      <td>8.594</td>\n",
       "      <td>8.250</td>\n",
       "      <td>7.656</td>\n",
       "      <td>-8.844</td>\n",
       "      <td>8.594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.281</td>\n",
       "      <td>7.438</td>\n",
       "      <td>7.125</td>\n",
       "      <td>8.562</td>\n",
       "      <td>7.406</td>\n",
       "      <td>7.094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.531</td>\n",
       "      <td>9.031</td>\n",
       "      <td>9.438</td>\n",
       "      <td>8.812</td>\n",
       "      <td>-7.719</td>\n",
       "      <td>-4.969</td>\n",
       "      <td>8.594</td>\n",
       "      <td>8.344</td>\n",
       "      <td>7.438</td>\n",
       "      <td>7.500</td>\n",
       "      <td>8.562</td>\n",
       "      <td>7.906</td>\n",
       "      <td>9.281</td>\n",
       "      <td>-6.562</td>\n",
       "      <td>7.438</td>\n",
       "      <td>7.188</td>\n",
       "      <td>8.344</td>\n",
       "      <td>-8.250</td>\n",
       "      <td>7.969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>8.719</td>\n",
       "      <td>7.844</td>\n",
       "      <td>7.156</td>\n",
       "      <td>6.938</td>\n",
       "      <td>7.438</td>\n",
       "      <td>7.125</td>\n",
       "      <td>7.438</td>\n",
       "      <td>9.781</td>\n",
       "      <td>8.438</td>\n",
       "      <td>8.625</td>\n",
       "      <td>9.438</td>\n",
       "      <td>7.781</td>\n",
       "      <td>4.625</td>\n",
       "      <td>7.969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.594</td>\n",
       "      <td>8.438</td>\n",
       "      <td>8.906</td>\n",
       "      <td>9.406</td>\n",
       "      <td>8.438</td>\n",
       "      <td>8.781</td>\n",
       "      <td>-6.750</td>\n",
       "      <td>-9.031</td>\n",
       "      <td>8.750</td>\n",
       "      <td>8.188</td>\n",
       "      <td>8.500</td>\n",
       "      <td>7.469</td>\n",
       "      <td>6.875</td>\n",
       "      <td>8.781</td>\n",
       "      <td>8.938</td>\n",
       "      <td>8.625</td>\n",
       "      <td>8.562</td>\n",
       "      <td>6.562</td>\n",
       "      <td>7.219</td>\n",
       "      <td>8.812</td>\n",
       "      <td>7.562</td>\n",
       "      <td>8.719</td>\n",
       "      <td>8.844</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>-8.312</td>\n",
       "      <td>7.844</td>\n",
       "      <td>8.906</td>\n",
       "      <td>8.500</td>\n",
       "      <td>8.375</td>\n",
       "      <td>8.938</td>\n",
       "      <td>8.281</td>\n",
       "      <td>8.781</td>\n",
       "      <td>8.781</td>\n",
       "      <td>7.562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2059 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "jokeId  5      7      8      13     15     16     17     18     19   20   \\\n",
       "userId                                                                     \n",
       "49941   0.0 -8.125 -7.844 -8.062 -8.281  7.406 -4.344 -5.594  5.625  0.0   \n",
       "49946   0.0 -9.875 -9.781 -9.812 -9.812 -9.875 -9.875 -9.781 -9.781  0.0   \n",
       "49961   0.0 -6.125  5.781  4.125 -0.656  2.219 -2.281  2.375  3.938  0.0   \n",
       "49963   0.0  6.531  0.188  4.688  1.719  5.688  4.438  4.812  3.125  0.0   \n",
       "49985   0.0 -4.406 -4.125 -5.469  1.594 -1.906 -1.062  1.469 -2.656  0.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "63944   0.0 -1.250  3.094  9.656  8.000 -5.375  4.438  6.781  3.188  0.0   \n",
       "63947   0.0 -5.906 -5.625 -6.438 -5.688 -0.125  3.156  3.156  7.438  0.0   \n",
       "63949   0.0 -5.094 -6.031  1.344  5.344  3.594 -6.406  3.062  3.062  0.0   \n",
       "63950   0.0  1.375  2.406  3.562  0.125  2.469  3.219  5.031  2.062  0.0   \n",
       "63978   0.0 -7.906 -7.594 -7.594 -6.375 -6.375 -6.375 -6.375 -1.812  0.0   \n",
       "\n",
       "jokeId    21     22     23     24     25     26   27     28     29     30   \\\n",
       "userId                                                                       \n",
       "49941  -3.250 -7.031 -5.812 -2.906 -6.719  0.031  0.0  6.250 -1.938 -2.375   \n",
       "49946   0.000  0.000 -9.781  0.000 -9.562  0.000  0.0  9.906  9.531  0.000   \n",
       "49961   3.156  0.000 -9.594  0.000  0.000  2.188  0.0  0.000  1.844  0.000   \n",
       "49963   0.312  9.781  4.125  0.219  6.500  7.250  0.0  9.781  1.719  1.312   \n",
       "49985  -4.781  0.000  0.000  0.000 -5.219 -2.844  0.0 -3.531  2.062 -4.781   \n",
       "...       ...    ...    ...    ...    ...    ...  ...    ...    ...    ...   \n",
       "63944   5.719  6.156  6.156  0.000  5.375  5.875  0.0  4.688  8.406  0.000   \n",
       "63947   0.719  0.000  0.000  0.000  0.000  8.938  0.0  8.781  0.000  0.000   \n",
       "63949  -5.562 -5.125 -5.594 -5.500 -5.125 -5.781  0.0  4.938 -6.375  5.281   \n",
       "63950   1.938  4.031  3.969 -4.969  2.125  0.375  0.0  4.781  5.062  2.156   \n",
       "63978  -5.438  9.156  8.844 -9.062  8.938  8.906  0.0  7.969  9.312  8.031   \n",
       "\n",
       "jokeId  31     32     33     34     35     36     37     38     39     40   \\\n",
       "userId                                                                       \n",
       "49941   0.0  5.812 -2.688  6.875  9.812  7.938 -8.281  4.719 -2.812  4.719   \n",
       "49946   0.0 -9.781  0.000 -6.625  9.156 -9.000  2.844  0.000  6.812  0.000   \n",
       "49961   0.0  4.844  0.000  0.000  4.812  1.000  0.000 -5.312 -6.812 -9.844   \n",
       "49963   0.0  0.469  1.312  2.250  8.531  6.094  9.719  9.812  7.875  2.094   \n",
       "49985   0.0 -2.094  0.000 -5.500 -1.875  4.500  0.000 -2.844 -2.125 -2.625   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944   0.0  6.000  4.688  4.688  2.812  9.812  0.000  1.656  7.188  8.031   \n",
       "63947   0.0  9.062  0.000  0.000  9.000  8.906  0.000  0.000  0.000  0.000   \n",
       "63949   0.0  3.750 -5.625 -5.125 -4.688  5.281  5.281  1.500  6.500  4.000   \n",
       "63950   0.0  1.125  1.438  3.594  1.156  3.594  0.000  6.938  2.188  3.438   \n",
       "63978   0.0  7.531 -7.469  7.500  6.938  8.969 -7.062 -5.156  9.156  8.375   \n",
       "\n",
       "jokeId    41     42   43     44     45     46     47     48     49     50   \\\n",
       "userId                                                                       \n",
       "49941  -6.688 -2.344  0.0 -2.156  6.750  4.531 -2.969 -2.719  2.719 -2.312   \n",
       "49946   0.000  9.344  0.0  0.000 -9.562 -9.656  8.656  4.531  8.906  0.000   \n",
       "49961   0.000  0.000  0.0  0.000 -9.469  0.000  2.406  4.000  1.531  6.594   \n",
       "49963   6.719 -0.125  0.0 -0.625  1.031  2.281 -0.062 -0.125  9.750  5.438   \n",
       "49985   0.000 -2.688  0.0  0.000  0.000  0.000  0.094 -9.469  4.438  2.500   \n",
       "...       ...    ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944   0.000  3.031  0.0  0.000  6.125 -0.438  6.594  0.000  6.656  7.625   \n",
       "63947   0.000  0.000  0.0  0.000  0.000  0.000  9.844  9.031  8.875  0.000   \n",
       "63949  -5.969 -5.750  0.0 -5.938  8.312 -5.594  5.375  6.219  5.906  5.688   \n",
       "63950  -4.188  1.781  0.0  6.531  4.156  4.000  1.625  4.188  3.875  3.031   \n",
       "63978   9.156  9.062  0.0 -8.438 -6.594  8.062  7.156  8.406  8.969  8.562   \n",
       "\n",
       "jokeId  51   52     53     54     55     56     57     58     59     60   61   \\\n",
       "userId                                                                          \n",
       "49941   0.0  0.0  7.125 -3.906 -7.875  6.781 -3.062  4.938  2.469 -3.625  0.0   \n",
       "49946   0.0  0.0  9.312 -9.750 -4.750  9.406  0.000  0.000  0.000  0.000  0.0   \n",
       "49961   0.0  0.0  3.125  5.031  0.000  3.719  0.000  0.000  0.000  0.000  0.0   \n",
       "49963   0.0  0.0  9.812  2.156  9.812 -6.719 -1.469  5.469  0.031  9.781  0.0   \n",
       "49985   0.0  0.0  4.656 -3.000  0.000 -1.906  0.000  0.000  0.000  0.000  0.0   \n",
       "...     ...  ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "63944   0.0  0.0  6.312  5.906  4.688  5.375  0.000 -5.031  4.219  0.906  0.0   \n",
       "63947   0.0  0.0  9.469  9.406  0.000  8.625  0.000  0.000  0.000  0.000  0.0   \n",
       "63949   0.0  0.0  1.125  1.250 -4.250 -2.500  0.625 -5.562  4.281 -5.781  0.0   \n",
       "63950   0.0  0.0  2.500  2.406  1.969  0.719  2.438  1.344 -0.031  3.031  0.0   \n",
       "63978   0.0  0.0  6.719  7.500 -7.750  8.500 -8.531 -8.656  8.844 -7.281  0.0   \n",
       "\n",
       "jokeId     62     63     64     65     66     67     68     69     70     71   \\\n",
       "userId                                                                          \n",
       "49941    2.812  0.094 -3.625 -4.188  7.594 -8.062 -1.031  2.062  5.406 -6.094   \n",
       "49946   10.000  9.844  0.000  0.000  5.719 -5.625  9.594 -0.844  6.188  0.000   \n",
       "49961    4.719  1.156  0.000 -9.594  4.375  0.000  0.688 -0.188  0.000  0.000   \n",
       "49963   -3.562  3.688 -1.156 -9.500 -1.000  9.781  5.875  9.156  7.312  9.812   \n",
       "49985    3.094 -3.781  0.000 -2.469  4.188  0.000 -2.500  3.500 -3.500  0.000   \n",
       "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944    4.562  4.594  3.750  4.344  5.375  0.000  6.938  3.031  5.844  0.000   \n",
       "63947    8.719  8.719  0.000  8.219  9.469  0.000  8.875  9.656  0.000  0.000   \n",
       "63949    6.156  6.219  4.000  1.906  5.500 -5.938  6.312 -3.656  4.375  7.031   \n",
       "63950    1.562  1.938 -2.031  3.688  5.469  4.000  4.656  2.719  3.344  0.969   \n",
       "63978    8.188  8.250 -8.188 -7.781  8.250 -6.812  8.594  8.250  7.656 -8.844   \n",
       "\n",
       "jokeId    72   73     74     75     76     77     78     79   80     81   \\\n",
       "userId                                                                     \n",
       "49941   8.344  0.0 -1.812  5.500  1.031  3.156 -1.156 -6.625  0.0  4.312   \n",
       "49946   9.875  0.0  0.000 -9.562  1.812  0.000 -9.594  0.000  0.0  0.000   \n",
       "49961   3.562  0.0  0.000  0.000  2.781  0.000 -7.062  0.000  0.0  2.781   \n",
       "49963   7.188  0.0  0.562  6.344  9.750  8.562  6.656  0.250  0.0  9.750   \n",
       "49985  -0.344  0.0  0.000  0.000 -3.312 -4.781 -2.812  0.000  0.0 -2.656   \n",
       "...       ...  ...    ...    ...    ...    ...    ...    ...  ...    ...   \n",
       "63944   7.156  0.0  0.000  0.000  4.344  0.000  3.531  0.000  0.0  4.969   \n",
       "63947   9.250  0.0  0.000  0.000  9.750  0.000  0.000  0.000  0.0  7.688   \n",
       "63949   0.812  0.0 -5.500  5.406  6.219  3.719 -5.875  6.906  0.0  5.500   \n",
       "63950   1.531  0.0  5.938  3.688  1.156  0.219  0.750  0.344  0.0  4.594   \n",
       "63978   8.594  0.0 -8.281  7.438  7.125  8.562  7.406  7.094  0.0  7.531   \n",
       "\n",
       "jokeId    82     83     84     85     86     87     88     89     90     91   \\\n",
       "userId                                                                         \n",
       "49941   2.938  7.000 -2.688 -6.500 -4.031  3.250 -2.469  0.406  1.344 -5.531   \n",
       "49946   0.000  0.000  0.000  0.000  6.375  0.000  0.000  9.656  0.000  2.719   \n",
       "49961   0.000  0.000  1.906  0.000  2.531  3.562 -5.781  5.969 -4.438  0.000   \n",
       "49963   1.250  1.656  7.406  0.125  0.594  0.375  6.969  9.688  5.406  1.406   \n",
       "49985  -3.719  0.000  1.500  0.000  2.562  2.781  3.219  4.719 -3.531 -2.469   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944   7.188  4.688  6.500  4.000  3.625  4.344  3.469  2.688  1.469  0.031   \n",
       "63947   0.000  8.000  0.000  0.000  0.000  7.438  0.000 -2.625  0.000  0.000   \n",
       "63949   5.281 -3.250 -4.562 -4.438  4.906 -5.750  0.719  5.781  3.750  5.500   \n",
       "63950  -4.312  3.250  0.188  2.938  2.719  6.188 -3.062  1.000 -4.250  1.031   \n",
       "63978   9.031  9.438  8.812 -7.719 -4.969  8.594  8.344  7.438  7.500  8.562   \n",
       "\n",
       "jokeId    92     93     94     95     96     97     98     99   100    101  \\\n",
       "userId                                                                       \n",
       "49941   5.656  6.094 -1.781 -2.875  7.719 -3.375  3.406 -6.625  0.0 -6.438   \n",
       "49946   9.375  4.312  3.438  5.750  0.000 -2.750  0.000  0.000  0.0  0.000   \n",
       "49961  -2.219  4.938  5.062  0.000  5.375  2.406  0.000  3.250  0.0  0.000   \n",
       "49963   9.781  5.469  4.781  0.188  0.375  8.562  0.156  9.812  0.0  7.500   \n",
       "49985   2.156 -3.656  1.562 -1.281  2.562 -3.531  0.000 -5.219  0.0  0.000   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...  ...    ...   \n",
       "63944   6.062  5.406  1.438  0.062  6.125  4.156  5.750  4.781  0.0  2.375   \n",
       "63947   9.219  9.094  0.000  0.000  0.000  7.844  8.344  0.000  0.0  0.000   \n",
       "63949   5.500  6.062  2.062  6.094  5.094  7.812 -5.781  4.688  0.0  6.906   \n",
       "63950   1.688  1.594  3.656  4.000  2.500  1.531  3.031 -3.188  0.0  3.125   \n",
       "63978   7.906  9.281 -6.562  7.438  7.188  8.344 -8.250  7.969  0.0 -7.062   \n",
       "\n",
       "jokeId    102    103    104    105    106    107    108    109    110    111  \\\n",
       "userId                                                                         \n",
       "49941   0.062  5.469  3.781  6.625  6.344 -3.094  2.375  7.938 -2.688  4.938   \n",
       "49946  -9.625 -9.656  9.500  9.938  9.875  0.000  7.969  7.906  9.750  9.938   \n",
       "49961   0.000 -7.062  7.031  3.750  4.938 -5.406  5.844 -2.906  5.219  5.625   \n",
       "49963   7.594  9.781  0.906  5.594  3.844  0.438  7.344  7.500  0.094  1.688   \n",
       "49985  -2.688  0.000  2.469  3.938 -3.531  2.656 -1.312  1.688 -1.219  0.625   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944   4.562 -0.688  7.156  6.750  5.375  4.969  6.312  4.812  2.656  1.500   \n",
       "63947   0.000  0.000  8.000 -0.344  8.000  0.000  9.625  8.938  8.719  9.219   \n",
       "63949   6.500  5.281  6.406 -1.875  5.594  6.219  3.312  7.625  8.281  6.156   \n",
       "63950   2.594  4.844  2.188  6.062  2.906  2.062  1.688  1.969 -1.719  0.938   \n",
       "63978   8.719  7.844  7.156  6.938  7.438  7.125  7.438  9.781  8.438  8.625   \n",
       "\n",
       "jokeId    112    113    114    115  116    117    118    119    120    121  \\\n",
       "userId                                                                       \n",
       "49941  -2.438  1.656  5.312  6.406  0.0  6.781  4.875  6.188  4.500  8.406   \n",
       "49946   9.969  9.781  9.875  9.844  0.0  9.750  9.719  9.844  9.500  9.562   \n",
       "49961  -2.281  0.000 -1.625 -5.094  0.0  3.625  3.438 -8.219  0.000 -9.375   \n",
       "49963   7.156  9.781  9.625  7.406  0.0  7.250  0.844  2.688  9.719  9.719   \n",
       "49985   2.156  0.000 -2.969  0.000  0.0 -0.750 -2.250 -2.281  0.000 -1.688   \n",
       "...       ...    ...    ...    ...  ...    ...    ...    ...    ...    ...   \n",
       "63944   6.062  4.375  7.406  0.000  0.0  3.438  4.969  7.250  0.000  4.719   \n",
       "63947   9.156  9.250  9.062  9.312  0.0  8.844  9.531  0.000  8.938  0.000   \n",
       "63949   7.625  5.875  6.344 -4.969  0.0  5.188  6.094 -2.812  6.969  5.219   \n",
       "63950   2.500  2.031  1.750  1.844  0.0  4.125  4.062  2.031 -3.906 -4.594   \n",
       "63978   9.438  7.781  4.625  7.969  0.0  7.594  8.438  8.906  9.406  8.438   \n",
       "\n",
       "jokeId    122    123    124    125    126    127    128    129    130    131  \\\n",
       "userId                                                                         \n",
       "49941  -2.062 -6.844 -5.969 -1.812  5.250  6.688  0.281  8.438  8.969  0.156   \n",
       "49946   9.250  0.000  0.000  0.000  8.719  9.688  9.312  9.906  8.125 -5.062   \n",
       "49961  -9.625  0.000  0.000  4.906 -5.438  2.125 -7.781  2.031  0.000  0.000   \n",
       "49963   7.406  4.531  2.750  5.406  0.625 -1.812  1.188  1.375 -0.375  7.906   \n",
       "49985  -5.219  0.000  0.000 -2.250  0.938 -1.312 -4.656  0.969  0.000  0.000   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944   3.969  0.000  0.000  3.375  5.375  6.531  4.594  2.375  6.156  0.000   \n",
       "63947   9.281  0.000  0.000  7.438  8.531 -0.375  9.875  9.875  0.000  0.000   \n",
       "63949   6.375 -5.969 -5.969  6.219  4.000  4.938  4.375  4.656  6.594  5.281   \n",
       "63950   1.031 -1.719 -7.031  1.781  3.375  5.688 -2.719  6.469 -1.656  5.719   \n",
       "63978   8.781 -6.750 -9.031  8.750  8.188  8.500  7.469  6.875  8.781  8.938   \n",
       "\n",
       "jokeId    132    133    134    135    136    137    138    139    140    141  \\\n",
       "userId                                                                         \n",
       "49941  -5.781 -0.344  8.500  2.469 -1.656 -2.062  0.594 -4.812 -6.938 -6.469   \n",
       "49946   0.000  7.531 -0.688 -9.531  4.719  0.000  6.812  8.156 -9.344  0.000   \n",
       "49961  -6.188  0.031  5.656 -9.906  0.000 -7.844  2.406  0.000 -2.750  0.000   \n",
       "49963   8.562  9.750  8.562  6.188  8.531  1.094  9.781  4.938 -7.062 -4.844   \n",
       "49985  -2.250 -2.844  2.312 -2.562  0.000 -4.656  1.844 -4.781 -3.531  0.000   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "63944   4.469  0.000  2.906  5.188  0.000  2.562  4.625  4.562  5.750  0.000   \n",
       "63947   5.625  9.219 -7.812  0.000  0.000  0.000  9.375  9.062  0.000  0.000   \n",
       "63949  -2.812  6.844  4.188  0.812  7.156 -5.781 -5.781  3.750 -6.469 -5.594   \n",
       "63950   1.969  1.938  3.062  4.656  6.625 -1.625  8.188  2.438 -3.438 -5.031   \n",
       "63978   8.625  8.562  6.562  7.219  8.812  7.562  8.719  8.844 -6.500 -8.312   \n",
       "\n",
       "jokeId    142    143    144    145    146    147    148    149    150  \n",
       "userId                                                                 \n",
       "49941  -0.594  5.938  8.125  5.125 -2.969 -3.688  0.969 -2.562  4.188  \n",
       "49946   0.000  6.156  9.844 -9.688  0.000  9.719  8.969 -9.688  9.688  \n",
       "49961   0.000  2.688  2.188  0.000  0.000  4.281  2.125 -3.562 -0.375  \n",
       "49963   4.312  1.438  7.594  9.781 -4.688 -2.469  2.031  6.969  1.219  \n",
       "49985   0.000  3.719 -3.688 -0.406  0.000  0.938 -1.938 -3.531  1.969  \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "63944   0.000  6.125  3.969  9.281  0.000  3.656  5.906  6.094  6.125  \n",
       "63947   8.938  9.531  0.000  9.562  0.000  9.438  9.156  8.781  8.781  \n",
       "63949   1.375  5.500  1.188  3.469 -5.500 -5.594  4.688  2.062  5.312  \n",
       "63950   2.000  2.375  0.875  3.625  2.281 -4.250  1.812  1.000  1.531  \n",
       "63978   7.844  8.906  8.500  8.375  8.938  8.281  8.781  8.781  7.562  \n",
       "\n",
       "[2059 rows x 140 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_user_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that finds similar users to a given Test user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes an input_test_user and neighbor_count as inputs. It first creates a row of the test user's interaction history with jokes and then finds the k nearest neighbors of the test user based on their interaction history with jokes using the KNN model trained earlier. It then prints the most similar users to the test user based on the calculated distances. Finally, it creates a list of similar user ids in the training data and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_users_for_testing(input_test_user, neighbor_count=best_n_neighbors+1):\n",
    "\n",
    "    test_user_row = jokes_user_test.loc[input_test_user].values.reshape(1, -1)\n",
    "    distances_test, indices_test = knn.kneighbors(test_user_row, n_neighbors=neighbor_count)\n",
    "\n",
    "\n",
    "    similaruser2=[]\n",
    "\n",
    "\n",
    "    # print('\\n Similar Users for User', input_test_user,' \\n')\n",
    "  \n",
    "    # Printing the most similar users to the input user based on the calculated distances\n",
    "    for i in range(1,len(distances_test.flatten())):\n",
    "    \n",
    "        #print( 'Most Similar User',i,': User_Id ' +  str(jokes_user_train.index[indices_test.flatten()[i]]),' with distance of ',str(distances_test.flatten()[i]) )\n",
    "        #Creating list for the similar users\n",
    "        similaruser2.append(jokes_user_train.index[indices_test.flatten()[i]])\n",
    "\n",
    "    \n",
    "    return similaruser2\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that gets the highest rated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the same with training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Jokes_with_Highhest_Rating_from_Similar_Users_for_testing(JokesfromSimilarUsers):\n",
    "\n",
    "    # Converting jokeId, jokeText and avg_rating_by_similar_users columns to lists\n",
    "    \n",
    "    jokeidlist=JokesfromSimilarUsers[\"jokeId\"].tolist()\n",
    "    ratelist= JokesfromSimilarUsers[\"avg_rating_by_similar_users\"].tolist()\n",
    "    textlist= JokesfromSimilarUsers[\"jokeText\"].tolist()\n",
    "\n",
    "    # Initialize variables to keep track of maximum rating and recommended joke\n",
    "    maxrating=0\n",
    "    recommended_joke2 = None\n",
    "\n",
    "    #Loop through the lists to find the joke with the highest rating\n",
    "    for i in range(0,len(jokeidlist)):\n",
    "        if (ratelist[i]> maxrating):\n",
    "            recommended_joke2=textlist[i]\n",
    "            maxrating=ratelist[i]\n",
    "\n",
    "    # Returning the recommended joke with the highest rating            \n",
    "    return recommended_joke2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that gets joke recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KNN_recommandations_for_user_for_testing(input_test_user, neighbor_count=best_n_neighbors+1):\n",
    "   \n",
    "\n",
    "    #Getting similar users to given input user\n",
    "    similaruser=None\n",
    "    similaruser=get_similar_users_for_testing(input_test_user,neighbor_count)\n",
    "\n",
    "    #Filtering the data to only include the rows of similar users\n",
    "    similar_user_data=None\n",
    "    similar_user_data = train_data[train_data[\"userId\"].isin(similaruser)]\n",
    "\n",
    "    #Computing the Average Rating of Jokes among the similar users \n",
    "    similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
    "\n",
    "\n",
    "    # Getting jokes rated by similar users\n",
    "    JokesfromSimilarUsers=None\n",
    "    JokesfromSimilarUsers= similar_user_data[[\"jokeId\",'jokeText','avg_rating_by_similar_users']].drop_duplicates(subset=['jokeId'])\n",
    "\n",
    "\n",
    "    # Getting the recommended joke with the highest average rating among similar user\n",
    "    recommended_joke2=None\n",
    "    recommended_joke2 = get_Jokes_with_Highhest_Rating_from_Similar_Users_for_testing(JokesfromSimilarUsers)\n",
    "\n",
    "\n",
    "    # Assigning rating values to the  variables that desired to print\n",
    "        # get average rating for recommended joke among similar users\n",
    "    RatingbySimilarUsers=JokesfromSimilarUsers[(JokesfromSimilarUsers['jokeText'] == recommended_joke2)]['avg_rating_by_similar_users'].iloc[0]\n",
    "\n",
    "    return recommended_joke2,RatingbySimilarUsers  \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((test_data['userId'] == 49963) & (test_data['jokeText'] == a)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>jokeId</th>\n",
       "      <th>rating</th>\n",
       "      <th>jokeText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1403360</th>\n",
       "      <td>49963</td>\n",
       "      <td>84</td>\n",
       "      <td>7.406</td>\n",
       "      <td>Q: What is the difference between Mechanical E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  jokeId  rating  \\\n",
       "1403360   49963      84   7.406   \n",
       "\n",
       "                                                  jokeText  \n",
       "1403360  Q: What is the difference between Mechanical E...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[(test_data['userId'] == 49963)& (test_data['jokeText'] == a) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.406000000000001"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[(test_data['userId'] == 49963) &(test_data['jokeText'] == a)]['rating'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test_row {'userid': 49985, 'joketext': 'Q: What is the difference between Mechanical Engineers and Civil \\nEngineers?\\n \\nA: Mechanical Engineers build weapons, Civil Engineers build targets.\\n', 'predicted_rating': 8.722, 'actual_rating': 1.5}  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "test_row = {'userid': 49985, 'joketext': a,'predicted_rating': RatingbySimilarUsers, 'actual_rating': test_data[(test_data['userId'] == 49985) &(test_data['jokeText'] == a)]['rating'].iloc[0]}\n",
    "print('\\n test_row', test_row,' \\n ' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLYING TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/3793230404.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_user_data['avg_rating_by_similar_users']  = similar_user_data.groupby('jokeId')['rating'].transform('mean')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  userid                                           joketext predicted_rating  \\\n",
      "0  63829  A group of girlfriends is on vacation when the...           4.2876   \n",
      "1  63839  Two attorneys went into a diner and ordered tw...            8.323   \n",
      "2  63859  A teacher is explaining to her class how diffe...         7.222333   \n",
      "3  63877  An old Scotsmen is sitting with a younger Scot...           6.4969   \n",
      "4  63882  A drunk staggers into a Catholic Church, enter...             8.35   \n",
      "\n",
      "  actual_rating  \n",
      "0         9.781  \n",
      "1         6.438  \n",
      "2         1.844  \n",
      "3         4.438  \n",
      "4         9.562  \n",
      "\n",
      " %  0.7285089849441476  of the test users are recommended a joke that they are already rated\n",
      "\n",
      " When comparing the predicted ratings and actual rating for those users, RMSE is calculated:   0.7285089849441476  of the test users are recommended a joke that they are already rated\n",
      "\n",
      " RMSE: 3.962298020141307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y7/_v7_fsx55jngwk83d1p07t8r0000gn/T/ipykernel_1462/2537026947.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_matrix = test_matrix.append(test_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#APPLYING TESTS\n",
    "test_matrix = pd.DataFrame(columns=['userid', 'joketext', 'predicted_rating','actual_rating'])\n",
    "\n",
    "#Getting Joke Recommandation to Every User in Test Data based on KNN\n",
    "\n",
    "user_ids_test = test_data['userId'].unique()\n",
    "user_ids_test.sort()\n",
    "user_ids_test=user_ids_test.tolist()\n",
    "\n",
    "for test_user_id in user_ids_test[-20:]:\n",
    "\n",
    "    test_user_row = jokes_user_test.loc[test_user_id].values.reshape(1, -1)\n",
    "    recommended_joke=None\n",
    "    RatingbySimilarUsers  =None\n",
    "    \n",
    "    recommended_joke, RatingbySimilarUsers  = get_KNN_recommandations_for_user_for_testing(test_user_id)\n",
    "\n",
    "\n",
    "    # Check if the actual rating for the recommended joke is non-zero\n",
    "    if ((test_data['userId'] == test_user_id) & (test_data['jokeText'] == recommended_joke)).any() and  test_data[(test_data['userId'] == test_user_id) & (test_data['jokeText'] == recommended_joke)]['rating'].values[0] != 0:\n",
    "        test_row = {'userid': test_user_id, 'joketext': recommended_joke,'predicted_rating': RatingbySimilarUsers, 'actual_rating': test_data[(test_data['userId'] == test_user_id) &(test_data['jokeText'] == recommended_joke)]['rating'].iloc[0]}\n",
    "        test_matrix = test_matrix.append(test_row, ignore_index=True)\n",
    "       # print('recommended movie is  ON THE LIST for user', test_user_id )\n",
    "\n",
    "    #else:\n",
    "       # print('recommended movie is not on the list for user', test_user_id)\n",
    "        \n",
    "\n",
    "# calculate the RMSE between the predicted ratings and actual ratings\n",
    "\n",
    "y_test = test_matrix['actual_rating']\n",
    "y_pred =test_matrix['predicted_rating']\n",
    "print(test_matrix.head())\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"\\n % \", ((test_matrix.shape[0]/len(user_ids_test))*100), ' of the test users are recommended a joke that they are already rated')\n",
    "print(\"\\n When comparing the predicted ratings and actual rating for those users, RMSE is calculated:  \", ((test_matrix.shape[0]/len(user_ids_test))*100), ' of the test users are recommended a joke that they are already rated')\n",
    "\n",
    "print(\"\\n RMSE:\", rmse)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Getting Joke Recommandation to Random User based on KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " random input is  60856\n",
      "\n",
      "dilay1\n",
      " \n",
      " distances are \n",
      "[0.15394149 0.15583797 0.17302237 0.17425702 0.18203175 0.19784699\n",
      " 0.205083   0.20730012 0.20832733 0.20908516 0.21120828]\n",
      "0.15583797228097918\n",
      " \n",
      " indices are\n",
      "[7938 6846 5948 6096 5822 6571 7223 6902 5729 2973 7850]\n",
      "6846\n",
      "\n",
      " Similar Users for User 60856  \n",
      "\n",
      "Most Similar User 1 : User_Id 43237  with distance of  0.15583797228097918\n",
      "Most Similar User 2 : User_Id 39141  with distance of  0.17302237342072024\n",
      "Most Similar User 3 : User_Id 39806  with distance of  0.17425701644257863\n",
      "Most Similar User 4 : User_Id 38587  with distance of  0.18203174872121075\n",
      "Most Similar User 5 : User_Id 41963  with distance of  0.1978469932594512\n",
      "Most Similar User 6 : User_Id 45283  with distance of  0.20508300454736184\n",
      "Most Similar User 7 : User_Id 43498  with distance of  0.20730012262009745\n",
      "Most Similar User 8 : User_Id 38207  with distance of  0.20832733191393893\n",
      "Most Similar User 9 : User_Id 19529  with distance of  0.20908516338663108\n",
      "Most Similar User 10 : User_Id 47846  with distance of  0.21120828129429503\n",
      "\n",
      "dilay2\n",
      "\n",
      "dilay3\n",
      "\n",
      "dilay4\n",
      "\n",
      "dilay5\n",
      "\n",
      "dilay8\n",
      "\n",
      "dilay9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yigit\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('An engineer, a physicist and a mathematician are sleeping in a\\nroom. There is a fire in the room. The engineer wakes up, sees the fire,\\npicks up the bucket of water and douses the fire and goes back to\\nsleep. \\n\\nAgain there is fire in the room. This time, the physicist wakes\\nup, notices the bucket, fills it with water, calculates the optimal\\ntrajectory and douses the fire in minimum amount of water and goes\\nback to sleep. \\n\\nAgain there is fire. This time the mathematician wakes up. \\nHe looks at the fire, looks at the bucket and the water and\\nexclaims, \"A solution exists\" and goes back to sleep.\\n',\n",
       " 8.06275)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the  list of unique user IDs in training data\n",
    "test_users = np.unique(test_data['userId'].values)\n",
    "test_users_sorted = sorted(test_users)\n",
    "# test_users_list = test_users_sorted.tolist()\n",
    "\n",
    "#Getting the Joke Recommandation based on KNN to a Random User from the list of User IDs \n",
    "randominput =np.random.choice(test_users_sorted)\n",
    "print('\\n random input is ',randominput)\n",
    "get_KNN_recommandations_for_user_for_testing(randominput)  #Input user id can be changed to any user id available in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLYING TO ONE TEST USER EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " distances are \n",
      "[0.48984949 0.49014196 0.49249364 0.49484695 0.50894253 0.50958816\n",
      " 0.51459717 0.51883234 0.51938623 0.52061704 0.52124159]\n",
      "0.4901419609108971\n",
      " \n",
      " indices are\n",
      "[1387 4635 8039 6144 5730 7061  571 1216 6433 1084 2243]\n",
      "4635\n",
      "\n",
      " Similar Users for User 49946  \n",
      "\n",
      "Most Similar User 1 : User_Id 33640  with distance of  0.4901419609108971\n",
      "Most Similar User 2 : User_Id 48696  with distance of  0.49249363706927063\n",
      "Most Similar User 3 : User_Id 40046  with distance of  0.49484694629071024\n",
      "Most Similar User 4 : User_Id 38208  with distance of  0.5089425296883762\n",
      "Most Similar User 5 : User_Id 44282  with distance of  0.5095881619757505\n",
      "Most Similar User 6 : User_Id 2219  with distance of  0.5145971735815109\n",
      "Most Similar User 7 : User_Id 6036  with distance of  0.5188323406028851\n",
      "Most Similar User 8 : User_Id 41316  with distance of  0.5193862275891342\n",
      "Most Similar User 9 : User_Id 5133  with distance of  0.5206170409235986\n",
      "Most Similar User 10 : User_Id 15016  with distance of  0.5212415937113255\n",
      "[[ 0.    -8.125 -7.844 -8.062 -8.281  7.406 -4.344 -5.594  5.625  0.\n",
      "  -3.25  -7.031 -5.812 -2.906 -6.719  0.031  0.     6.25  -1.938 -2.375\n",
      "   0.     5.812 -2.688  6.875  9.812  7.938 -8.281  4.719 -2.812  4.719\n",
      "  -6.688 -2.344  0.    -2.156  6.75   4.531 -2.969 -2.719  2.719 -2.312\n",
      "   0.     0.     7.125 -3.906 -7.875  6.781 -3.062  4.938  2.469 -3.625\n",
      "   0.     2.812  0.094 -3.625 -4.188  7.594 -8.062 -1.031  2.062  5.406\n",
      "  -6.094  8.344  0.    -1.812  5.5    1.031  3.156 -1.156 -6.625  0.\n",
      "   4.312  2.938  7.    -2.688 -6.5   -4.031  3.25  -2.469  0.406  1.344\n",
      "  -5.531  5.656  6.094 -1.781 -2.875  7.719 -3.375  3.406 -6.625  0.\n",
      "  -6.438  0.062  5.469  3.781  6.625  6.344 -3.094  2.375  7.938 -2.688\n",
      "   4.938 -2.438  1.656  5.312  6.406  0.     6.781  4.875  6.188  4.5\n",
      "   8.406 -2.062 -6.844 -5.969 -1.812  5.25   6.688  0.281  8.438  8.969\n",
      "   0.156 -5.781 -0.344  8.5    2.469 -1.656 -2.062  0.594 -4.812 -6.938\n",
      "  -6.469 -0.594  5.938  8.125  5.125 -2.969 -3.688  0.969 -2.562  4.188]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.490141960910897"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# APPLYING TO ONE TEST USER EXAMPLE\n",
    "input_test_user_id= 49946\n",
    "test_user_row = jokes_user_test.loc[49941].values.reshape(1, -1)\n",
    "distancestest, indicestest = knn.kneighbors(test_user_row, n_neighbors=best_n_neighbors+1)\n",
    "\n",
    "\n",
    "print(' \\n distances are ')\n",
    "print(distancestest.flatten())\n",
    "print(distancestest.flatten()[1])\n",
    "\n",
    "print(' \\n indices are')\n",
    "\n",
    "print(indicestest.flatten())\n",
    "print(indicestest.flatten()[1])\n",
    "\n",
    "\n",
    "\n",
    "print('\\n Similar Users for User', input_test_user_id,' \\n')\n",
    "  \n",
    " # Printing the most similar users to the input user based on the calculated distances\n",
    "for i in range(1,len(distancestest.flatten())):\n",
    "    \n",
    "    print( 'Most Similar User',i,': User_Id ' +  str(jokes_user_train.index[indicestest.flatten()[i]]),' with distance of ',str(distancestest.flatten()[i]) )\n",
    "\n",
    "\n",
    "\n",
    "    #Creating list for the similar users\n",
    "    similaruser2=[]\n",
    "    for i in range(0,len(distances.flatten())):\n",
    "        similaruser2.append(jokes_user_train.index[indicestest.flatten()[i]])\n",
    "        \n",
    "print(test_user_row)\n",
    "jokes_user_train.loc[33640].values.reshape(1, -1)\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "a=jokes_user_test.loc[49941].values.reshape(1, -1)\n",
    "cosine(a, jokes_user_train.loc[33640].values.reshape(1, -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is a widely used matrix factorization technique in machine learning that can be used for various tasks, including image compression, data compression, and recommender systems. It decomposes a matrix into three separate matrices, each representing the latent factors of the original matrix. \n",
    "\n",
    "In this assignment, a RecSys has been built using the Jester Price dataset, which contains user ratings for jokes. The implemented RecSys uses the SVD algorithm to identify similar jokes based on a user's highest rated joke and recommend them to the user.\n",
    "\n",
    "The SVD algorithm works by decomposing the user-joke matrix into three separate matrices, each representing the latent factors of the original matrix. This factorization allows us to identify similar jokes based on the users' ratings, rather than just their content. To begin, the recommendation system takes the user ID as input and identifies the highest rated joke by that user. The SVD model is then applied to identify similar jokes to the user's highest rated joke, and these similar jokes are recommended to the user.\n",
    "\n",
    "This implementation offers a powerful and accurate approach to recommend jokes to users based on their preferences, as identified by the SVD algorithm. The RecSys has the potential to enhance user engagement and satisfaction by providing personalized recommendations that match their sense of humor. The algorithm can also be further improved by incorporating other features such as the text content of jokes, to provide even more accurate recommendations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.1 Building RecSys based on SVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creating a function for calculating cosine similarity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is a measure of similarity between two non-zero vectors that measures the cosine of the angle between them. It ranges from -1 (completely dissimilar) to 1 (completely similar), with 0 indicating no similarity.  Jokes can be represented as feature vectors, where each feature represents a characteristic of the joke. By comparing the feature vectors of different jokes, we can identify jokes that are similar to each other and recommend them to users who enjoy similar types of humor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for calculating cosine similarity\n",
    "\n",
    "def cosine_similarity(v,u):\n",
    "    # Calculate the dot product of v and u, and divide by the product of their magnitudes\n",
    "    return (v @ u)/ (np.linalg.norm(v) * np.linalg.norm(u))  #The @ symbol is used to calculate the dot product of v and u "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Singular value decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7664\\2574634037.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Singular value decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Singular value decomposition\n",
    "u, s, vt = svd(matrix, full_matrices=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creating a function for calculating cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_jokes(userId, joke_index, num_jokes=10):\n",
    "    # Calculate the cosine similarity between the given joke and all other jokes in the user-item matrix\n",
    "    similarities = [cosine_similarity(vt[joke_index], vt[i]) for i in range(vt.shape[0])]\n",
    "\n",
    "    # Getting the list of the jokes rated (aka heard) by the user  \n",
    "    user_ratedjokes = train_data[(train_data['userId'] == userId)]['jokeId'].tolist()\n",
    "\n",
    "  # Get the indices of the top similar jokes, excluding jokes already rated by the user\n",
    "    similar_jokes_ids = []\n",
    "    for i in np.argsort(similarities)[::-1]:\n",
    "        joke_ids = i+1  #Since  (jokes_user_train.columns)[0]=joke id 1\n",
    "        if  joke_ids not in user_ratedjokes: # excluding jokes already rated by the user\n",
    "            similar_jokes_ids.append(joke_ids)\n",
    "        if len(similar_jokes_ids) == num_jokes:\n",
    "            break \n",
    "    return similar_jokes_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a function that recommends a joke for the User"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function get_SVD_recommandations_for_user takes an input user ID and joke count as inputs. The default value for recommended joke count is set to 1.\n",
    "\n",
    "It first finds the highest rated joke by the given user. To do this, it sorts the user ratings in descending order and retrieves the joke ID   and title of the highest rated joke  from the first row.\n",
    "\n",
    "Then, the function calls the get_similar_jokes() function with the user ID and highest rated joke ID as input parameters to find similar jokes. \n",
    "\n",
    "The function then prints the top (num_jokes) number of similar jokes for the user to the console using a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SVD_recommandations_for_user(userId, num_jokes=1):\n",
    "\n",
    "    # Find the highest rated joke by the given user\n",
    "        # Get all ratings made by the given user\n",
    "    user_ratings = train_data[train_data['userId'] == userId]\n",
    "        # Sort the ratings in descending order to find the highest rated joke\n",
    "    user_ratings_sorted = user_ratings.sort_values(['rating'], ascending=[False])\n",
    "        # Get the joke ID of the highest rated joke by the user\n",
    "    highest_rated_joke_id = user_ratings_sorted.iloc[0]['jokeId']\n",
    "        # Get the title of the highest rated joke\n",
    "    highest_rated_joke_title = train_data[train_data['jokeId'] == highest_rated_joke_id]['jokeText'].values[0]\n",
    "        # Print the title of the highest rated joke by the user\n",
    "    print(\"Highest rated  joke by user\", userId, \"is:\", highest_rated_joke_title)\n",
    "    \n",
    "    # Get similar jokes based on the highest rated joke\n",
    "    similar_jokes=None\n",
    "    similar_jokes= get_similar_jokes(userId,highest_rated_joke_id)\n",
    "\n",
    "    \n",
    "    # Print the top 'num_jokes' number of similar jokes for the user\n",
    "    print(\"Top\", num_jokes, \"similar jokes for user \", userId , ': ')\n",
    "    for i, jokeId in enumerate(similar_jokes):\n",
    "        print(i+1, \".\", similar_jokes[i], ' ',  train_data[train_data['jokeId'] == similar_jokes[i]]['jokeText'].values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Getting Joke Recommandation to Random User based on SVD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code selects a random user id from the list of unique user ids. The get_SVD_recommandations_for_user() function is then called with this randomly selected user id as the input to generate a joke recommendation for the selected user using the KNN algorithm.\n",
    "\n",
    "The get_KNN_recommandations_for_user() function first finds the highest rated joke by a given user, and then finds similar jokes based on that highest rated joke. It then prints the highest rated joke and the top recommended jokes to the console.\n",
    "\n",
    "This code allows for easy testing of the recommendation algorithm on different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Getting the Joke Recommandation based on SVD to a Random User from the list of User IDs \n",
    "\n",
    "get_SVD_recommandations_for_user (np.random.choice(user_ids))  #Input user id can be changed to any user id available in the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.2 Evaulation of SVD Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_jokes2(joke_index, num_jokes=10):\n",
    "    \n",
    "    \n",
    "    # Calculate the cosine similarity between the given joke and all other joke in the user-item matrix\n",
    "    similarities = [cosine_similarity(vt[joke_index], vt[i]) for i in range(vt.shape[0])]\n",
    "    \n",
    "    # Get the indices of the top similar jokes\n",
    "    similar_joke_indices = np.argsort(similarities)[::-1][1:num_jokes+1]\n",
    "    \n",
    "    # Get the ids of the top similar jokes\n",
    "    similar_jokes_ids = [list(jokes_user_train.columns)[i] for i in similar_joke_indices]\n",
    "    \n",
    "    return similar_jokes_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SVD_recommandations_for_user2(userId, num_jokes=10):\n",
    "\n",
    "    # Find the highest rated joke by the given user\n",
    "    user_ratings = train_data[train_data['userId'] == userId]\n",
    "    user_ratings_sorted = user_ratings.sort_values(['rating'], ascending=[False])\n",
    "    highest_rated_joke_id = user_ratings_sorted.iloc[0]['jokeId']\n",
    "    highest_rated_joke_title = train_data[train_data['jokeId'] == highest_rated_joke_id]['jokeText'].values[0]\n",
    "    print(\"Highest rated  joke by user\", userId, \"is:\", highest_rated_joke_title)\n",
    "    \n",
    "    # Find the similar jokes to the   highest rated joke by the given user\n",
    "    similar_jokes= get_similar_jokes2(highest_rated_joke_id)\n",
    "\n",
    "    # Print the top similar jokes\n",
    "    print(\"Top\", num_jokes, \"similar jokes for user \", userId , ': ')\n",
    "    for i, jokeId in enumerate(similar_jokes):\n",
    "        print(i+1, \".\", similar_jokes[i], ' ',  train_data[train_data['jokeId'] == similar_jokes[i]]['jokeText'].values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6. Answering Business Questions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = spars_train.nonzero()[0][user_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88904114496b5f829e1478e7923ec9775e8603956a22d180c120ee5a0c432294"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
